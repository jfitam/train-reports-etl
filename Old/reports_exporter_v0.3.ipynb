{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70d7d56-b9c1-4f36-bacb-cacaa0828e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V 0.1 \n",
    "# basic functionalities. read and export Train List, Occupancy, and Booking Payment Detailed\n",
    "# \n",
    "# V 0.2 \n",
    "# handle all the information of each kind of report together. \n",
    "#\n",
    "# v 0.3\n",
    "# add logging\n",
    "# delete all the information in the same query\n",
    "# added importation by chunks\n",
    "# added information of the process of each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23eb465f-e7a6-4825-ac11-7b330b4ccf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to the database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine(\"postgresql+psycopg2://postgres:Renfe2022@172.19.28.174:5433/SalesSystem\", pool_recycle=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a1be9b-b12b-4563-93d1-245c0a6902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# STATES\n",
    "NO_REPORT = 0\n",
    "TRAIN_LIST_REPORT = 1\n",
    "OCCUPANCY_REPORT = 2\n",
    "BOOKING_PAYMENT_REPORT = 3\n",
    "\n",
    "# tables\n",
    "train_list_table = 'train_list'\n",
    "occupancy_table = 'occupancy_list_hist'\n",
    "bpd_table = 'booking_payment_detailed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff81a607-e938-4a64-96ac-eb017940aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "log_name = 'exportation_' + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M') + '.log'\n",
    "logging.basicConfig(filename=log_name, filemode='w', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605a995c-d1f1-4539-87d6-94396745528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version control\n",
    "version = 0.3\n",
    "max_control = 0\n",
    "\n",
    "# check if the current version is the last one\n",
    "query = \"SELECT version FROM \\\"AFC\\\".exporter_version_control\"\n",
    "all_versions = pd.read_sql_query(query, alchemyEngine)\n",
    "max_version = all_versions['version'].max()\n",
    "\n",
    "\n",
    "if version > max_version:\n",
    "    # add the new version\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        query = text(f\"insert into \\\"AFC\\\".exporter_version_control(date, version) values (\\'{datetime.datetime.now()}\\',\\'{version}\\')\")\n",
    "        conn.execute(query)\n",
    "        conn.commit()\n",
    "    \n",
    "elif version < max_version:\n",
    "    # this program is out of date, terminating execution\n",
    "    sys.exit(\"Current exporter is out of date. Please, use the last version to export data.\")\n",
    "    \n",
    "\n",
    "#else:\n",
    "    #current is the last version. Do nothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e33501a-967f-47ed-9241-96921eda2e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version\n",
       "0      0.0\n",
       "1      0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a166319-2d83-41fd-a721-eb6728effe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prt_info(string, kind='info', nl=True):\n",
    "    # Get the current time\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    info = f\"[{current_time}] {string}\"\n",
    "\n",
    "    info = '\\r'+info\n",
    "    \n",
    "    # Handle different kinds of logging\n",
    "    if kind == 'info':\n",
    "        logging.info(info)\n",
    "    elif kind == 'warning':\n",
    "        logging.warning(info)\n",
    "    elif kind == 'error':\n",
    "        logging.error(info)\n",
    "    elif kind == 'critical':\n",
    "        logging.critical(info)\n",
    "\n",
    "    # Print the information\n",
    "    print(info, end='' if not nl else '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a3f3f76-9e01-4666-91c6-45ac0eb035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that detects which kind of report is the excel file\n",
    "def get_report_name(excel_file_path):\n",
    "\n",
    "    train_list_header = pd.DataFrame([\n",
    "        'Departure Date',\n",
    "        'Train Number',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Coach Number',\n",
    "        'Seat Number',\n",
    "        'Class',\n",
    "        'Booking Code',\n",
    "        'Ticket Number',\n",
    "        'Tariff',\n",
    "        'Status',\n",
    "        'Payment Mode',\n",
    "        'Media Type',\n",
    "        'Sales Channel',\n",
    "        'Base Price',\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff',\n",
    "        'Amount Not Refunded',\n",
    "        'Compensation Type',\n",
    "        'Compensation Reason',\n",
    "        'Compensation Status',\n",
    "        'Nationality',\n",
    "        'Gender',\n",
    "        'Name',\n",
    "        'Surname',\n",
    "        'Document',\n",
    "        'Prefix',\n",
    "        'Telephone',\n",
    "        'Profile',\n",
    "        'Special Needs',\t\n",
    "        'Validation Time',\n",
    "        'Group',\n",
    "        'Checked On Board',\n",
    "        'Last Operation Channel',\n",
    "        'Last Operation Equipment Code'\n",
    "        ])\n",
    "\n",
    "    occupancy_header = pd.DataFrame([\n",
    "        'Date',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Train ID',\n",
    "        'Train Number',\n",
    "        'Class',\n",
    "        'Total Seats (Quota + Carer + PRM)',\n",
    "        'Quota Configuration',\n",
    "        'Total Locks (Quota + Carer + PRM)',\n",
    "        'For Sale',\n",
    "        'Reserved Usual Seats',\n",
    "        'Reserved PRM Seats',\n",
    "        'Reserved Carer Seats',\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)',\n",
    "        'Reserved & Lock Usual Seats',\n",
    "        'Reserved & Lock PRM Seats',\n",
    "        'Reserved & Lock Carer Seats',\t\n",
    "        'Total Available',\n",
    "        'Validating',\n",
    "        'No Show',\n",
    "        'UnBooked',\t\n",
    "        'Passengers Inc. Infants',\n",
    "        'Checked On Board'\n",
    "    ])\n",
    "\n",
    "    bpd_header = pd.DataFrame([\n",
    "       'Booking Code',\n",
    "       'Ticket Number',\t\n",
    "       'Operation Date',\t\n",
    "       'Base Price',\n",
    "       'VAT Base Price',\n",
    "       'Management Fee',\n",
    "       'VAT Management Fee',\n",
    "       'Payment Fee',\n",
    "       'VAT Payment Fee',\n",
    "       'Operation Amount',\t\n",
    "       'Penalty Tariff',\t\n",
    "       'Compensation Type',\t\n",
    "       'Compensation Reason',\t\n",
    "       'Compensation Status',\n",
    "       'Card Number',\n",
    "       'Authorization Code',\n",
    "       'Order ID',\n",
    "       'Transaction ID',\n",
    "       'Status Payment Card',\n",
    "       'Card Brand',\n",
    "       'Bill Number',\n",
    "       'Bill Status',\n",
    "       'Train Number',\t\n",
    "       'Departure Date',\t\n",
    "       'Arrival Date',\n",
    "       'OD',\n",
    "       'Origin Station',\n",
    "       'Destination Station',\n",
    "       'Class',\n",
    "       'Tariff',\t\n",
    "       'Reserved Number of Seats',\n",
    "       'Status',\n",
    "       'Card Serial Number',\n",
    "       'Card User Name',\n",
    "       'Sales Station',\n",
    "       'Sales Channel',\n",
    "       'Sales Equipment Code',\n",
    "       'Payment Mode',\n",
    "       'Coach Number',\t\n",
    "       'Seat Number',\n",
    "       'Nationality',\n",
    "       'Name',\n",
    "       'Surname',\n",
    "       'Gender',\n",
    "       'Document Type',\n",
    "       'Document',\n",
    "       'Prefix',\n",
    "       'Telephone',\n",
    "       'Email',\n",
    "       'Profile',\t\n",
    "       'Validation Time',\n",
    "       'Checked On Board',\t\n",
    "       'Detail Type',\n",
    "       'Tipology',\n",
    "       'Last Operation Channel',\n",
    "       'Last Operation Equipment Code'\n",
    "\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # read the header of the file\n",
    "        file_header = pd.read_excel(excel_file_path, skiprows=6, nrows=2, header=None)\n",
    "    except Exception as e:\n",
    "        prt_info(f\"There is a problem reading the file: {e}\")\n",
    "        return NO_REPORT\n",
    "        \n",
    "    # clean\n",
    "    file_header = file_header.transpose()\n",
    "    file_header_n7 = file_header[0]\n",
    "    file_header_n8 = file_header[1]\n",
    "\n",
    "    file_header_n7.dropna(axis=0, inplace=True)\n",
    "    file_header_n7.reset_index(drop=True, inplace=True)\n",
    "    file_header_n7 = pd.DataFrame(file_header_n7)\n",
    "    file_header_n7.columns = [0]\n",
    "\n",
    "    file_header_n8.dropna(axis=0, inplace=True)\n",
    "    file_header_n8.reset_index(drop=True, inplace=True)\n",
    "    file_header_n8 = pd.DataFrame(file_header_n8)\n",
    "    file_header_n8.columns = [0]\n",
    "    \n",
    "    # comparision\n",
    "    if(pd.DataFrame(file_header_n8).equals(train_list_header)): return TRAIN_LIST_REPORT\n",
    "    elif(pd.DataFrame(file_header_n7).equals(occupancy_header)): return OCCUPANCY_REPORT\n",
    "    elif(pd.DataFrame(file_header_n8).equals(bpd_header)): return BOOKING_PAYMENT_REPORT\n",
    "    else: return NO_REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b18b48-847c-4889-9e93-a112717e12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_list(file_name, alchemyEngine):\n",
    "    # function to read the train_list excel file and calculate the extra columns of the report\n",
    "    \n",
    "    # train_list datatype\n",
    "    train_list_datatype = {\n",
    "    'Departure Date': str,\n",
    "    'Train Number': str,\n",
    "    'OD': str,\n",
    "    'Origin Station': str,\n",
    "    'Destination Station': str,\n",
    "    'Coach Number': str,\n",
    "    'Seat Number': str,\n",
    "    'Class': str,\n",
    "    'Booking Code': str,\n",
    "    'Ticket Number': str,\n",
    "    'Tariff': str,\n",
    "    'Status': str,\n",
    "    'Payment Mode': str,\n",
    "    'Media Type': str,\n",
    "    'Sales Channel': str,\n",
    "    'Base Price': float,\n",
    "    'VAT Base Price': float,\n",
    "    'Management Fee': float,\n",
    "    'VAT Management Fee': float,\n",
    "    'Payment Fee': float,\n",
    "    'VAT Payment Fee': float,\n",
    "    'Operation Amount':\tfloat,\n",
    "    'Penalty Tariff': float,\n",
    "    'Amount Not Refunded': float,\n",
    "    'Compensation Type': str,\n",
    "    'Compensation Reason': str,\n",
    "    'Compensation Status': str,\n",
    "    'Nationality': str,\n",
    "    'Gender': str,\n",
    "    'Name': str,\n",
    "    'Surname': str,\n",
    "    'Document': str,\n",
    "    'Prefix': str,\n",
    "    'Telephone': str,\n",
    "    'Profile': str,\n",
    "    'Special Needs': str,\t\n",
    "    'Validation Time': str,\n",
    "    'Group': str,\n",
    "    'Checked On Board': str,\n",
    "    'Last Operation Channel': str,\n",
    "    'Last Operation Equipment Code': str\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # open file\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=7, dtype=train_list_datatype\n",
    "                                #,\n",
    "                                #parse_dates=['Departure Date', 'Validation Time'],\n",
    "                                #date_format={'Departure Date': '%Y-%m-%d %H:%M', 'Validation Time': '%Y-%m-%d  %H:%M'}\n",
    "                               )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "    \n",
    "    # Remove the last row\n",
    "    df_file = df_file.drop(df_file.index[-1])\n",
    "\n",
    "    # check duplicates\n",
    "    #duplicates = df_file[\"Ticket Number\"].duplicated(keep='first')\n",
    "    #if(duplicates.sum() > 0):\n",
    "    #    prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "    #    df_file.drop_duplicates(subset='Ticket Number', keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    # format current date columns\n",
    "    departure_date_time = pd.to_datetime(df_file['Departure Date'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_file['Departure Date'] = departure_date_time.dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_file['Validation Time'] = pd.to_datetime(df_file['Validation Time'], format=\"%Y-%m-%d %H:%M:%S\").dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "    \n",
    "    # create extra columns\n",
    "    df_file['Train_hour'] = departure_date_time.dt.strftime('%H:%M')\n",
    "    df_file['Departure_Date_Short'] = departure_date_time.dt.strftime('%Y-%m-%d')\n",
    "    df_file['Train-OD Short'] = df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "    df_file['CORRIDOR'] = df_file['Train Number'].str[:2]\n",
    "    df_file['WEEK_DAY'] = departure_date_time.dt.strftime('%a')\n",
    "    df_file['WEEK_NUM'] = departure_date_time.dt.isocalendar().week\n",
    "    df_file['train_key'] = df_file['Departure_Date_Short'] + \" - \" + df_file['Train-OD Short']\n",
    "    \n",
    "    # get the train departure\n",
    "    try:\n",
    "        train_hours = pd.read_sql_table('train_departure_times', alchemyEngine, schema='AFC')\n",
    "    except Exception as e:\n",
    "        raise(f\"Error fetching the departure times from database: {e}\")\n",
    "        \n",
    "    train_hours.columns = ['Train Number', 'train_departure_date_time']\n",
    "    df_file = pd.merge(df_file, train_hours, on=\"Train Number\", how=\"left\")\n",
    "\n",
    "    #check if there is missing hours for the train numbers of this file\n",
    "    if(df_file['train_departure_date_time'].isnull().sum() > 0):\n",
    "        trains_missing = df_file[df_file['train_departure_date_time'].isnull()]['Train Number'].unique()\n",
    "        raise Exception(f\"There are missing departing hours in the database. Please, check the following trains: {\", \".join(trains_missing)}\")\n",
    "\n",
    "    # calculate the departing time of the train\n",
    "    df_file['train_departure_date_time'] = pd.to_datetime(df_file['Departure_Date_Short'].astype(str) + \" \" + df_file['train_departure_date_time'].astype(str))\n",
    "    train_date_adjustment = df_file['train_departure_date_time'].dt.time > departure_date_time.dt.time\n",
    "    df_file['train_departure_date_time'] = df_file['train_departure_date_time'] - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "    df_file['train_departure_date_short'] = departure_date_time.dt.date - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "    \n",
    "    # calculate the services date (reduce one day if it is an early train before maintenance window)\n",
    "    service_date_adjustment = df_file['train_departure_date_time'].dt.time <= datetime.time(5, 0)\n",
    "    df_file['Service_Date'] = df_file['train_departure_date_short'] - pd.to_timedelta(service_date_adjustment.astype(int), unit=\"D\")\n",
    "    \n",
    "    # get the date time of the operation\n",
    "    ticket_numbers = \", \".join(f\"'{ticket}'\" for ticket in df_file['Ticket Number'].unique())\n",
    "    query = f\"\"\"\n",
    "    SELECT ticket_number AS \\\"Ticket Number\\\", operation_date_time\n",
    "    FROM \\\"AFC\\\".booking_payment_detailed\n",
    "    WHERE ticket_number IN ({ticket_numbers})\n",
    "    \"\"\"\n",
    "    df_operation_date_times = pd.read_sql_query(query, alchemyEngine)\n",
    "    df_file = pd.merge(df_file, df_operation_date_times, on=\"Ticket Number\", how=\"left\")\n",
    "    df_file['operation_date'] = df_file['operation_date_time'].dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # set the headers according to database\n",
    "    df_file.columns = [\n",
    "    'departure_date', \n",
    "    'train_number', \n",
    "    'od', \n",
    "    'origin_station', \n",
    "    'destination_station',\n",
    "    'coach_number', \n",
    "    'seat_number', \n",
    "    'class', \n",
    "    'booking_code', \n",
    "    'ticket_number', \n",
    "    'tariff', \n",
    "    'status', \n",
    "    'payment_mode', \n",
    "    'media_type', \n",
    "    'sales_channel', \n",
    "    'base_price', \n",
    "    'vat_base_price',\n",
    "    'management_fee', \n",
    "    'vat_management_fee', \n",
    "    'payment_fee', \n",
    "    'vat_payment_fee', \n",
    "    'operation_amount', \n",
    "    'penalty_tariff', \n",
    "    'amount_not_refunded', \n",
    "    'compensation_type', \n",
    "    'compensation_reason', \n",
    "    'compensation_status', \n",
    "    'nationality', \n",
    "    'gender', \n",
    "    'name', \n",
    "    'surname', \n",
    "    'document', \n",
    "    'prefix', \n",
    "    'telephone', \n",
    "    'profile', \n",
    "    'special_needs', \n",
    "    'validating_time', \n",
    "    'groupyn', \n",
    "    'checked_on_board', \n",
    "    'last_operation_channel', \n",
    "    'last_operation_equipment_code', \n",
    "    'train_hour', \n",
    "    'departure_date_short', \n",
    "    'train_od_short', \n",
    "    'stretch', \n",
    "    'week_day', \n",
    "    'week_num', \n",
    "    'train_key', \n",
    "    'train_departure_date_time', \n",
    "    'train_departure_date_short', \n",
    "    'service_train_departure_date_short', \n",
    "    'operation_date_time', \n",
    "    'operation_date']\n",
    "\n",
    "    return df_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab08a1d6-63fb-4744-b4f6-ed06ff87f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_booking_payment(file_name):\n",
    "\n",
    "    booking_payment_datatype = {\n",
    "        'Booking Code':str,\n",
    "       'Ticket Number':str,\t\n",
    "       'Operation Date':str,\t\n",
    "       'Base Price':float,\n",
    "       'VAT Base Price':float,\n",
    "       'Management Fee':float,\n",
    "       'VAT Management Fee':float,\n",
    "       'Payment Fee':float,\n",
    "       'VAT Payment Fee':float,\n",
    "       'Operation Amount':float,\t\n",
    "       'Penalty Tariff':float,\t\n",
    "       'Compensation Type':str,\t\n",
    "       'Compensation Reason':str,\t\n",
    "       'Compensation Status':str,\n",
    "       'Card Number':str,\n",
    "       'Authorization Code':str,\n",
    "       'Order ID':str,\n",
    "       'Transaction ID':str,\n",
    "       'Status Payment Card':str,\n",
    "       'Card Brand':str,\n",
    "       'Bill Number':str,\n",
    "       'Bill Status':str,\n",
    "       'Train Number':str,\t\n",
    "       'Departure Date':str,\t\n",
    "       'Arrival Date':str,\n",
    "       'OD':str,\n",
    "       'Origin Station':str,\n",
    "       'Destination Station':str,\n",
    "       'Class':str,\n",
    "       'Tariff':str,\t\n",
    "       'Reserved Number of Seats':str,\n",
    "       'Status':str,\n",
    "       'Card Serial Number':str,\n",
    "       'Card User Name':str,\n",
    "       'Sales Station':str,\n",
    "       'Sales Channel':str,\n",
    "       'Sales Equipment Code':str,\n",
    "       'Payment Mode':str,\n",
    "       'Coach Number':str,\t\n",
    "       'Seat Number':str,\n",
    "       'Nationality':str,\n",
    "       'Name':str,\n",
    "       'Surname':str,\n",
    "       'Gender':str,\n",
    "       'Document Type':str,\n",
    "       'Document':str,\n",
    "       'Prefix':str,\n",
    "       'Telephone':str,\n",
    "       'Email':str,\n",
    "       'Profile':str,\t\n",
    "       'Validation Time':str,\n",
    "       'Checked On Board':str,\t\n",
    "       'Detail Type':str,\n",
    "       'Tipology':str,\n",
    "       'Last Operation Channel':str,\n",
    "       'Last Operation Equipment Code':str\n",
    "    }\n",
    "\n",
    "    # read\n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=7, dtype=booking_payment_datatype)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "\n",
    "    # Remove the last row\n",
    "    df_file = df_file.drop(df_file.index[-1])\n",
    "    \n",
    "    # check duplicates\n",
    "    #duplicates = df_file[\"Ticket Number\"].duplicated(keep='first')\n",
    "    #if(duplicates.sum() > 0):\n",
    "    #    prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "    #    df_file.drop_duplicates(subset='Ticket Number', keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    # format the dates\n",
    "    df_file['Operation Date'] = pd.to_datetime(df_file['Operation Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_file['Departure Date'] = pd.to_datetime(df_file['Departure Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_file['Arrival Date'] = pd.to_datetime(df_file['Arrival Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # set column names\n",
    "    df_file.columns = [\n",
    "        'booking_code', \n",
    "        'ticket_number',\n",
    "        'operation_date_time', \n",
    "        'base_price', \n",
    "        'base_price_vat', \n",
    "        'management_fee', \n",
    "        'management_fee_vat', \n",
    "        'payment_fee', \n",
    "        'payment_fee_vat', \n",
    "        'operation_amount', \n",
    "        'penalty_tariff', \n",
    "        'amount_not_refunded', \n",
    "        'compensation_type', \n",
    "        'compensation_reason', \n",
    "        'compensation_status', \n",
    "        'card_number', \n",
    "        'authorization_code', \n",
    "        'order_id', \n",
    "        'transaction_id', \n",
    "        'status_payment_card', \n",
    "        'card_brand', \n",
    "        'bill_number', \n",
    "        'bill_status', \n",
    "        'train_number', \n",
    "        'departure_date_time', \n",
    "        'arrival_date_time', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'class', \n",
    "        'tariff', \n",
    "        'reserved_number_of_seats', \n",
    "        'status', \n",
    "        'card_serial_number', \n",
    "        'card_user_name', \n",
    "        'sales_station', \n",
    "        'sales_channel', \n",
    "        'equipment_code', \n",
    "        'payment_mode', \n",
    "        'coach_number', \n",
    "        'seat_number', \n",
    "        'country_code', \n",
    "        'name', \n",
    "        'surname', \n",
    "        'gender', \n",
    "        'document_type', \n",
    "        'document', \n",
    "        'prefix', \n",
    "        'telephone', \n",
    "        'email', \n",
    "        'profile', \n",
    "        'validating_time', \n",
    "        'checked_on_board', \n",
    "        'detail_type', \n",
    "        'tipology', \n",
    "        #'compensated', \n",
    "        #'include_fare_revenue', \n",
    "        'last_operation_channel', \n",
    "        'last_operation_equipment_code'\n",
    "    ]\n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4684c4-cb9e-4a89-9ffc-9843f34cb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_occupancy(file_name):    \n",
    "    \n",
    "    # define the datatype\n",
    "    occupancy_datatype = {\n",
    "        'Date':str,\n",
    "        'OD':str,\n",
    "        'Origin Station':str,\n",
    "        'Destination Station':str,\n",
    "        'Train ID':str,\n",
    "        'Train Number':str,\n",
    "        'Class':str,\n",
    "        'Total Seats (Quota + Carer + PRM)':str,\n",
    "        'Quota Configuration':str,\n",
    "        'Total Locks (Quota + Carer + PRM)':str,\n",
    "        'For Sale':str,\n",
    "        'Reserved Usual Seats':str,\n",
    "        'Reserved PRM Seats':str,\n",
    "        'Reserved Carer Seats':str,\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)':str,\n",
    "        'Reserved & Lock Usual Seats':str,\n",
    "        'Reserved & Lock PRM Seats':str,\n",
    "        'Reserved & Lock Carer Seats':str,\t\n",
    "        'Total Available':str,\n",
    "        'Validating':str,\n",
    "        'No Show':str,\n",
    "        'UnBooked':str,\t\n",
    "        'Passengers Inc. Infants':str,\n",
    "        'Checked On Board':str\n",
    "    }\n",
    "    \n",
    "    # read the file\n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=6, dtype=occupancy_datatype, parse_dates=['Date'], date_format={'Date':'%Y-%m-%d %H:%M:%S'})\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "    \n",
    "    # Remove the last two row\n",
    "    df_file.drop(df_file.index[-1], inplace=True)\n",
    "    df_file.drop(df_file.index[-1], inplace=True)\n",
    "        \n",
    "    # reformat date columns\n",
    "    date_time = pd.to_datetime(df_file['Date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df_file['Date'] = date_time.dt.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # create the extra columns\n",
    "    df_file['Data_Date'] = datetime.date.today()\n",
    "    df_file['train_key'] = date_time.dt.strftime('%Y-%m-%d') + \" - \" + df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "    \n",
    "    # rename the columns\n",
    "    df_file.columns = [\n",
    "        'date', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'train_id', \n",
    "        'train_number', \n",
    "        'class', \n",
    "        'total_seats', \n",
    "        'quota_configuration', \n",
    "        'total_locks', \n",
    "        'for_sale', \n",
    "        'reserved_usual_seats', \n",
    "        'reserved_prm_seats', \n",
    "        'reserved_carer_seats', \n",
    "        'ticket_reserved', \n",
    "        'reserved_lock_usual_seats', \n",
    "        'reserved_lock_prm_seats', \n",
    "        'reserved_lock_carer_seats', \n",
    "        'total_available', \n",
    "        'validating', \n",
    "        'no_show', \n",
    "        'unbooked', \n",
    "        'passengers_inc_infant', \n",
    "        'checked_on_board', \n",
    "        'data_date', \n",
    "        'train_key'\n",
    "    ]\n",
    "    \n",
    "\n",
    "        \n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3819a751-b9a7-482d-91be-a9e949d90040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each pair of dates (beginning, end) of the streak days in the submitted list of dates\n",
    "def get_date_pairs(df, column):\n",
    "    date_pairs = []\n",
    "    date_col = np.sort(pd.to_datetime(df[column]).dt.date.unique())\n",
    "    date_begin = date_col.min()\n",
    "    date_end = date_col.min()\n",
    "    day_count = (date_col.max() - date_col.min()).days + 1\n",
    "    \n",
    "    # if there is a single date, return that\n",
    "    if(len(date_col) == 1):\n",
    "        return [[date_col.min(), date_col.max()]]\n",
    "    \n",
    "    # iterate through the dates\n",
    "    for d in date_col:\n",
    "        #skip the first date\n",
    "        if(d == date_col.min()): continue\n",
    "    \n",
    "        # check if it is continous\n",
    "        if((d - date_end).days == 1):\n",
    "            date_end = d\n",
    "        else:\n",
    "            date_pairs.append([date_begin.strftime('%Y-%m-%d'), date_end.strftime('%Y-%m-%d')])\n",
    "            date_begin = d\n",
    "            date_end = d\n",
    "    #at the end insert the last element\n",
    "    if(date_begin is not None):\n",
    "        date_pairs.append([date_begin.strftime('%Y-%m-%d'), date_end.strftime('%Y-%m-%d')])\n",
    "        \n",
    "\n",
    "    return date_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f419de-7c5c-4ace-9685-ed0391bd58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_train_list(df_file, alchemyEngine):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    unique_dates = df_file['departure_date_short'].unique()\n",
    "    date_conditions = ', '.join([f\"'{date}'\" for date in unique_dates])\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = train_list_table\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        \n",
    "        # get the dates of the record\n",
    "        date_pairs = get_date_pairs(df_file, 'departure_date_short')\n",
    "\n",
    "        # notice a warning if there are missing dates in the middle of the data\n",
    "        if(len(date_pairs) >1):\n",
    "            prt_info(\"The dates on the report Train List are not consecutive. Make sure all the files of the day has been submitted\", 'warning')\n",
    "        \n",
    "        # delete the previous records\n",
    "        for date_from, date_to in date_pairs:\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE departure_date_short between \\'{date_from}\\' and \\'{date_to}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                #prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while deleting the previous data for date {date}: {e}\")\n",
    "\n",
    "        # insert the data day by day\n",
    "        for date, group in df_file.groupby('departure_date_short'):\n",
    "            # Insert the data for the current date\n",
    "            chunk_size = 500\n",
    "            try:\n",
    "                for chunk in range(0, len(group), chunk_size):\n",
    "                    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                    conn.commit()\n",
    "                    prt_info(f\"Data for {date}: {chunk} entries inserted.\", nl=False)\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a140b83-233b-4805-ac86-488fc1bc1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_booking_payment(df_file, alchemyEngine):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['operation_date_time'], format=\"%Y-%m-%d %H:%M\").dt.date\n",
    "    unique_dates = dates.unique()\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = bpd_table\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        conn.autocommit = True\n",
    "\n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Delete existing records for the current date\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(operation_date_time, 'yyyy-mm-dd') = \\'{date}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                #prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while deleting the previous data for date {date}: {e}\")\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            chunk_size = 500\n",
    "            try:\n",
    "                for chunk in range(0, len(group), chunk_size):\n",
    "                    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                    conn.commit()\n",
    "                    prt_info(f\"Data for {date}: {chunk} entries inserted.\", nl=False)\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a690c858-dae7-4111-b639-7f4290fcc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_occupancy(df_file, alchemyEngine):\n",
    "  \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['date']).dt.strftime('%Y-%m-%d')\n",
    "    unique_dates = dates.sort_values().unique()\n",
    "    today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = occupancy_table\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        # get the dates of the record\n",
    "        date_pairs = get_date_pairs(df_file, 'date')\n",
    "\n",
    "        # notice a warning if there are missing dates in the middle of the data\n",
    "        if(len(date_pairs) >1):\n",
    "            prt_info(\"The dates on the report Train List are not consecutive. Make sure all the files of the day has been submitted\", 'warning')\n",
    "        \n",
    "        # delete the previous records\n",
    "        for date_from, date_to in date_pairs:\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(date, 'yyyy-mm-dd') between \\'{date_from}\\' and \\'{date_to}\\'and data_date = \\'{today}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                #prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while deleting the previous data for date {date}: {e}\")\n",
    "    \n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            chunk_size = 500\n",
    "            try:\n",
    "                for chunk in range(0, len(group), chunk_size):\n",
    "                    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                    conn.commit()\n",
    "                    prt_info(f\"Data for {date}: {chunk} entries inserted.\", nl=False)\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3216f5e-a3f2-41b5-a793-1b6ae279a5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-14 11:22:00] \n",
      "**************** Exporter version 0.3 ***********************\n",
      "[2024-08-14 11:22:00] Date 2024-08-14 11:22:00.564392\n",
      "[2024-08-14 11:22:00] Found excel file P09_TL_2024-09-15_to_2024-09-30_@_2024_08_14.nopag.xlsx as Train List\n",
      "[2024-08-14 11:22:00] Reading Train List...\n",
      "[2024-08-14 11:22:05] P09_TL_2024-09-15_to_2024-09-30_@_2024_08_14.nopag.xlsx read.\n",
      "[2024-08-14 11:22:05] Deleting 4 duplicated entries.\n",
      "[2024-08-14 11:22:05] Exporting Train List...\n",
      "[2024-08-14 11:22:16] Data for 2024-09-15 inserted successfully (534 inserted).\n",
      "[2024-08-14 11:22:16] Data for 2024-09-16 inserted successfully (432 inserted).\n",
      "[2024-08-14 11:22:16] Data for 2024-09-17 inserted successfully (530 inserted).\n",
      "[2024-08-14 11:22:16] Data for 2024-09-18 inserted successfully (466 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-19 inserted successfully (383 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-20 inserted successfully (273 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-21 inserted successfully (361 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-22 inserted successfully (232 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-23 inserted successfully (306 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-24 inserted successfully (277 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-25 inserted successfully (212 inserted).\n",
      "[2024-08-14 11:22:17] Data for 2024-09-26 inserted successfully (147 inserted).\n",
      "[2024-08-14 11:22:18] Data for 2024-09-27 inserted successfully (125 inserted).\n",
      "[2024-08-14 11:22:18] Data for 2024-09-28 inserted successfully (243 inserted).\n",
      "[2024-08-14 11:22:18] Data for 2024-09-29 inserted successfully (156 inserted).\n",
      "[2024-08-14 11:22:18] Data for 2024-09-30 inserted successfully (113 inserted).\n",
      "[2024-08-14 11:22:18] Report Train List exported successfully.\n",
      "[2024-08-14 11:22:18] Exportation finished.\n"
     ]
    }
   ],
   "source": [
    "# ************************************* MAIN PROGRAM *****************************************************\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "prt_info(f\"\\n**************** Exporter version {version} ***********************\")\n",
    "prt_info(f\"Date {datetime.datetime.today()}\")\n",
    "files_found = {}\n",
    "\n",
    "# get all xlsx files\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        \n",
    "        # check if it is a report\n",
    "        kind_file = get_report_name(file)\n",
    "        \n",
    "        if(kind_file != NO_REPORT):\n",
    "            # get the name\n",
    "            if(kind_file == TRAIN_LIST_REPORT): kind_file_name = 'Train List'\n",
    "            elif(kind_file == BOOKING_PAYMENT_REPORT): kind_file_name = 'Booking Payment Detailed'\n",
    "            elif(kind_file == OCCUPANCY_REPORT): kind_file_name = 'Occupancy'\n",
    "            else: kind_file_name = 'Unknown'\n",
    "            prt_info(f\"Found excel file {file} as {kind_file_name}\")\n",
    "\n",
    "            # add the file and the kind to the list\n",
    "            if not kind_file_name in files_found:\n",
    "                files_found[kind_file_name] = []\n",
    "                \n",
    "            files_found[kind_file_name].append(file)\n",
    "        \n",
    "        else: prt_info(f\"Found file {file} but could not detect any report in it\")\n",
    "\n",
    "# export each report one by one\n",
    "for report in files_found:\n",
    "    df = pd.DataFrame()\n",
    "    prt_info(f\"Reading {report}...\")\n",
    "    \n",
    "    # check all the files associated\n",
    "    for file in files_found[report]:\n",
    "\n",
    "        # reading files\n",
    "        try:\n",
    "            is_read = True\n",
    "            if(report == 'Train List'): df_file = read_train_list(file, alchemyEngine)\n",
    "            elif(report == 'Booking Payment Detailed'): df_file = read_booking_payment(file)\n",
    "            elif(report == 'Occupancy'): df_file = read_occupancy(file)            \n",
    "            else:\n",
    "                prt_info(f\"Reading of files {report} have not been implemented yet.\")\n",
    "                is_read = False\n",
    "\n",
    "            if is_read:\n",
    "                prt_info(f\"{file} read.\")\n",
    "                df = pd.concat([df, df_file])\n",
    "                \n",
    "        except Exception as e:\n",
    "            prt_info(e)\n",
    "            prt_info(f\"Reading of file {file} failed.\")\n",
    "            files_found[report].remove(file)\n",
    "            continue\n",
    "\n",
    "    # if there is data\n",
    "    if df.empty:\n",
    "        prt_info(f'No data to export for report {report}')\n",
    "    else:\n",
    "        # order the dataframe\n",
    "        if(report == 'Train List'): sort_by = ['departure_date', 'operation_date_time']\n",
    "        elif(report == 'Booking Payment Detailed'): sort_by = ['operation_date_time']\n",
    "        elif(report == 'Occupancy'): sort_by = ['ticket_reserved', 'quota_configuration']\n",
    "            \n",
    "        df.sort_values(by= sort_by, ascending=True, inplace=True)\n",
    "    \n",
    "        # remove duplicates\n",
    "        if(report == 'Train List'): subset_col = ['ticket_number']\n",
    "        elif(report == 'Booking Payment Detailed'): subset_col = None\n",
    "        elif(report == 'Occupancy'): subset_col = ['date', 'od','train_number', 'class']\n",
    "    \n",
    "        if subset_col is not None:\n",
    "            duplicates = df.duplicated(subset=subset_col, keep='last')\n",
    "            if(duplicates.sum() > 0):\n",
    "                prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "                df.drop_duplicates(subset=subset_col, keep='last', inplace=True, ignore_index=True)\n",
    "            \n",
    "        # export the valid files\n",
    "        prt_info(f\"Exporting {report}...\")\n",
    "        try:\n",
    "            if(report == 'Train List'):\n",
    "                export_train_list(df, alchemyEngine)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            elif(report == 'Booking Payment Detailed'):\n",
    "                export_booking_payment(df, alchemyEngine)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            elif(report == 'Occupancy'):\n",
    "                export_occupancy(df, alchemyEngine)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            else:\n",
    "                prt_info(f\"Exportation of report {report} have not been implemented yet.\")\n",
    "        except Exception as e:\n",
    "            prt_info(e)\n",
    "            prt_info(\"Exportation failed. Exportation of the report aborted.\")\n",
    "            continue\n",
    "\n",
    "        # save the results\n",
    "        df.to_csv(f\"{report} data exported {datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}.csv\")\n",
    "\n",
    "prt_info(\"Exportation finished.\")\n",
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
