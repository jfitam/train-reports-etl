{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b70d7d56-b9c1-4f36-bacb-cacaa0828e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V0.1 basic functionalities. read and export Train List, Occupancy, and Booking Payment Detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b181ec-a1fa-4d28-9b06-6b26881ee558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\jfita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\jfita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\jfita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\jfita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.32)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\jfita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\jfita\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "!pip install psycopg2\n",
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47067baf-1f7c-4136-8216-07f8d02500d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to the database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine(\"postgresql+psycopg2://postgres:Renfe2022@172.19.28.174:5433/SalesSystem\", pool_recycle=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39a1be9b-b12b-4563-93d1-245c0a6902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# STATES\n",
    "NO_REPORT = 0\n",
    "TRAIN_LIST_REPORT = 1\n",
    "OCCUPANCY_REPORT = 2\n",
    "BOOKING_PAYMENT_REPORT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a166319-2d83-41fd-a721-eb6728effe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that just add a timestamps decoraton in print functions\n",
    "def prt_info(string):\n",
    "    import datetime\n",
    "    \n",
    "    time = datetime.datetime.now()\n",
    "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a3f3f76-9e01-4666-91c6-45ac0eb035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that detects which kind of report is the excel file\n",
    "def get_report_name(excel_file_path):\n",
    "\n",
    "    train_list_header = pd.DataFrame([\n",
    "        'Departure Date',\n",
    "        'Train Number',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Coach Number',\n",
    "        'Seat Number',\n",
    "        'Class',\n",
    "        'Booking Code',\n",
    "        'Ticket Number',\n",
    "        'Tariff',\n",
    "        'Status',\n",
    "        'Payment Mode',\n",
    "        'Media Type',\n",
    "        'Sales Channel',\n",
    "        'Base Price',\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff',\n",
    "        'Amount Not Refunded',\n",
    "        'Compensation Type',\n",
    "        'Compensation Reason',\n",
    "        'Compensation Status',\n",
    "        'Nationality',\n",
    "        'Gender',\n",
    "        'Name',\n",
    "        'Surname',\n",
    "        'Document',\n",
    "        'Prefix',\n",
    "        'Telephone',\n",
    "        'Profile',\n",
    "        'Special Needs',\t\n",
    "        'Validation Time',\n",
    "        'Group',\n",
    "        'Checked On Board',\n",
    "        'Last Operation Channel',\n",
    "        'Last Operation Equipment Code'\n",
    "        ])\n",
    "\n",
    "    occupancy_header = pd.DataFrame([\n",
    "        'Date',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Train ID',\n",
    "        'Train Number',\n",
    "        'Class',\n",
    "        'Total Seats (Quota + Carer + PRM)',\n",
    "        'Quota Configuration',\n",
    "        'Total Locks (Quota + Carer + PRM)',\n",
    "        'For Sale',\n",
    "        'Reserved Usual Seats',\n",
    "        'Reserved PRM Seats',\n",
    "        'Reserved Carer Seats',\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)',\n",
    "        'Reserved & Lock Usual Seats',\n",
    "        'Reserved & Lock PRM Seats',\n",
    "        'Reserved & Lock Carer Seats',\t\n",
    "        'Total Available',\n",
    "        'Validating',\n",
    "        'No Show',\n",
    "        'UnBooked',\t\n",
    "        'Passengers Inc. Infants',\n",
    "        'Checked On Board'\n",
    "    ])\n",
    "\n",
    "    bpd_header = pd.DataFrame([\n",
    "       'Booking Code',\n",
    "       'Ticket Number',\t\n",
    "       'Operation Date',\t\n",
    "       'Base Price',\n",
    "       'VAT Base Price',\n",
    "       'Management Fee',\n",
    "       'VAT Management Fee',\n",
    "       'Payment Fee',\n",
    "       'VAT Payment Fee',\n",
    "       'Operation Amount',\t\n",
    "       'Penalty Tariff',\t\n",
    "       'Compensation Type',\t\n",
    "       'Compensation Reason',\t\n",
    "       'Compensation Status',\n",
    "       'Card Number',\n",
    "       'Authorization Code',\n",
    "       'Order ID',\n",
    "       'Transaction ID',\n",
    "       'Status Payment Card',\n",
    "       'Card Brand',\n",
    "       'Bill Number',\n",
    "       'Bill Status',\n",
    "       'Train Number',\t\n",
    "       'Departure Date',\t\n",
    "       'Arrival Date',\n",
    "       'OD',\n",
    "       'Origin Station',\n",
    "       'Destination Station',\n",
    "       'Class',\n",
    "       'Tariff',\t\n",
    "       'Reserved Number of Seats',\n",
    "       'Status',\n",
    "       'Card Serial Number',\n",
    "       'Card User Name',\n",
    "       'Sales Station',\n",
    "       'Sales Channel',\n",
    "       'Sales Equipment Code',\n",
    "       'Payment Mode',\n",
    "       'Coach Number',\t\n",
    "       'Seat Number',\n",
    "       'Nationality',\n",
    "       'Name',\n",
    "       'Surname',\n",
    "       'Gender',\n",
    "       'Document Type',\n",
    "       'Document',\n",
    "       'Prefix',\n",
    "       'Telephone',\n",
    "       'Email',\n",
    "       'Profile',\t\n",
    "       'Validation Time',\n",
    "       'Checked On Board',\t\n",
    "       'Detail Type',\n",
    "       'Tipology',\n",
    "       'Last Operation Channel',\n",
    "       'Last Operation Equipment Code'\n",
    "\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        # read the header of the file\n",
    "        file_header = pd.read_excel(excel_file_path, skiprows=6, nrows=2, header=None)\n",
    "    except Exception as e:\n",
    "        prt_info(f\"There is a problem reading the file: {e}\")\n",
    "        return NO_REPORT\n",
    "        \n",
    "    # clean\n",
    "    file_header = file_header.transpose()\n",
    "    file_header_n7 = file_header[0]\n",
    "    file_header_n8 = file_header[1]\n",
    "\n",
    "    file_header_n7.dropna(axis=0, inplace=True)\n",
    "    file_header_n7.reset_index(drop=True, inplace=True)\n",
    "    file_header_n7 = pd.DataFrame(file_header_n7)\n",
    "    file_header_n7.columns = [0]\n",
    "\n",
    "    file_header_n8.dropna(axis=0, inplace=True)\n",
    "    file_header_n8.reset_index(drop=True, inplace=True)\n",
    "    file_header_n8 = pd.DataFrame(file_header_n8)\n",
    "    file_header_n8.columns = [0]\n",
    "    \n",
    "    # comparision\n",
    "    if(pd.DataFrame(file_header_n8).equals(train_list_header)): return TRAIN_LIST_REPORT\n",
    "    elif(pd.DataFrame(file_header_n7).equals(occupancy_header)): return OCCUPANCY_REPORT\n",
    "    elif(pd.DataFrame(file_header_n8).equals(bpd_header)): return BOOKING_PAYMENT_REPORT\n",
    "    else: return NO_REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34b18b48-847c-4889-9e93-a112717e12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_list(file_name, alchemyEngine):\n",
    "    # function to read the train_list excel file and calculate the extra columns of the report\n",
    "    \n",
    "    # train_list datatype\n",
    "    train_list_datatype = {\n",
    "    'Departure Date': str,\n",
    "    'Train Number': str,\n",
    "    'OD': str,\n",
    "    'Origin Station': str,\n",
    "    'Destination Station': str,\n",
    "    'Coach Number': str,\n",
    "    'Seat Number': str,\n",
    "    'Class': str,\n",
    "    'Booking Code': str,\n",
    "    'Ticket Number': str,\n",
    "    'Tariff': str,\n",
    "    'Status': str,\n",
    "    'Payment Mode': str,\n",
    "    'Media Type': str,\n",
    "    'Sales Channel': str,\n",
    "    'Base Price': float,\n",
    "    'VAT Base Price': float,\n",
    "    'Management Fee': float,\n",
    "    'VAT Management Fee': float,\n",
    "    'Payment Fee': float,\n",
    "    'VAT Payment Fee': float,\n",
    "    'Operation Amount':\tfloat,\n",
    "    'Penalty Tariff': float,\n",
    "    'Amount Not Refunded': float,\n",
    "    'Compensation Type': str,\n",
    "    'Compensation Reason': str,\n",
    "    'Compensation Status': str,\n",
    "    'Nationality': str,\n",
    "    'Gender': str,\n",
    "    'Name': str,\n",
    "    'Surname': str,\n",
    "    'Document': str,\n",
    "    'Prefix': str,\n",
    "    'Telephone': str,\n",
    "    'Profile': str,\n",
    "    'Special Needs': str,\t\n",
    "    'Validation Time': str,\n",
    "    'Group': str,\n",
    "    'Checked On Board': str,\n",
    "    'Last Operation Channel': str,\n",
    "    'Last Operation Equipment Code': str\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # open file\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=7, dtype=train_list_datatype,\n",
    "                                parse_dates=['Departure Date', 'Validation Time'],\n",
    "                                date_format={'Departure Date': '%Y-%m-%d %H:%M', 'Validation Time': '%Y-%m-%d  %H:%M'})\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "    \n",
    "    # Remove the last row\n",
    "    df_file = df_file.drop(df_file.index[-1])\n",
    "\n",
    "    # check duplicates\n",
    "    duplicates = df_file[\"Ticket Number\"].duplicated(keep='first')\n",
    "    if(duplicates.sum() > 0):\n",
    "        prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "        df_file.drop_duplicates(subset='Ticket Number', keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    # format current date columns\n",
    "    departure_date_time = pd.to_datetime(df_file['Departure Date'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_file['Departure Date'] = departure_date_time.dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_file['Validation Time'] = pd.to_datetime(df_file['Validation Time'], format=\"%Y-%m-%d %H:%M:%S\").dt.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # create extra columns\n",
    "    df_file['Train_hour'] = departure_date_time.dt.strftime('%H:%M')\n",
    "    df_file['Departure_Date_Short'] = departure_date_time.dt.strftime('%Y-%m-%d')\n",
    "    df_file['Train-OD Short'] = df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "    df_file['CORRIDOR'] = df_file['Train Number'].str[:2]\n",
    "    df_file['WEEK_DAY'] = departure_date_time.dt.strftime('%a')\n",
    "    df_file['WEEK_NUM'] = departure_date_time.dt.isocalendar().week\n",
    "    df_file['train_key'] = df_file['Departure_Date_Short'] + \" - \" + df_file['Train-OD Short']\n",
    "    \n",
    "    # get the train departure\n",
    "    try:\n",
    "        train_hours = pd.read_sql_table('train_departure_times', alchemyEngine, schema='AFC')\n",
    "    except Exception as e:\n",
    "        raise(f\"Error fetching the departure times from database: {e}\")\n",
    "        \n",
    "    train_hours.columns = ['Train Number', 'train_departure_date_time']\n",
    "    df_file = pd.merge(df_file, train_hours, on=\"Train Number\", how=\"inner\")\n",
    "\n",
    "    #check if there is missing hours for the train numbers of this file\n",
    "    if(df_file['train_departure_date_time'].isnull().sum() > 0):\n",
    "        trains_missing = df_file[df_file['train_departure_date_time'].isnull()]['Train Number'].unique()\n",
    "        raise Exception(f\"There are missing departing hours in the database. Please, check the following trains: {\", \".join(trains_missing)}\")\n",
    "\n",
    "    # calculate the departing time of the train\n",
    "    df_file['train_departure_date_time'] = pd.to_datetime(df_file['Departure_Date_Short'].astype(str) + \" \" + df_file['train_departure_date_time'].astype(str))\n",
    "    train_date_adjustment = df_file['train_departure_date_time'].dt.time > departure_date_time.dt.time\n",
    "    df_file['train_departure_date_time'] = df_file['train_departure_date_time'] - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "    df_file['train_departure_date_short'] = departure_date_time.dt.date - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "    \n",
    "    # calculate the services date (reduce one day if it is an early train before maintenance window)\n",
    "    service_date_adjustment = df_file['train_departure_date_time'].dt.time <= datetime.time(5, 0)\n",
    "    df_file['Service_Date'] = df_file['train_departure_date_short'] - pd.to_timedelta(service_date_adjustment.astype(int), unit=\"D\")\n",
    "    \n",
    "    # get the date time of the operation\n",
    "    ticket_numbers = \", \".join(f\"'{ticket}'\" for ticket in df_file['Ticket Number'].unique())\n",
    "    query = f\"\"\"\n",
    "    SELECT ticket_number AS \\\"Ticket Number\\\", operation_date_time\n",
    "    FROM \\\"AFC\\\".booking_payment_detailed\n",
    "    WHERE ticket_number IN ({ticket_numbers})\n",
    "    \"\"\"\n",
    "    df_operation_date_times = pd.read_sql_query(query, alchemyEngine)\n",
    "    df_file = pd.merge(df_file, df_operation_date_times, on=\"Ticket Number\", how=\"left\")\n",
    "    df_file['operation_date'] = df_file['operation_date_time'].dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # set the headers according to database\n",
    "    df_file.columns = [\n",
    "    'departure_date', \n",
    "    'train_number', \n",
    "    'od', \n",
    "    'origin_station', \n",
    "    'destination_station',\n",
    "    'coach_number', \n",
    "    'seat_number', \n",
    "    'class', \n",
    "    'booking_code', \n",
    "    'ticket_number', \n",
    "    'tariff', \n",
    "    'status', \n",
    "    'payment_mode', \n",
    "    'media_type', \n",
    "    'sales_channel', \n",
    "    'base_price', \n",
    "    'vat_base_price',\n",
    "    'management_fee', \n",
    "    'vat_management_fee', \n",
    "    'payment_fee', \n",
    "    'vat_payment_fee', \n",
    "    'operation_amount', \n",
    "    'penalty_tariff', \n",
    "    'amount_not_refunded', \n",
    "    'compensation_type', \n",
    "    'compensation_reason', \n",
    "    'compensation_status', \n",
    "    'nationality', \n",
    "    'gender', \n",
    "    'name', \n",
    "    'surname', \n",
    "    'document', \n",
    "    'prefix', \n",
    "    'telephone', \n",
    "    'profile', \n",
    "    'special_needs', \n",
    "    'validating_time', \n",
    "    'groupyn', \n",
    "    'checked_on_board', \n",
    "    'last_operation_channel', \n",
    "    'last_operation_equipment_code', \n",
    "    'train_hour', \n",
    "    'departure_date_short', \n",
    "    'train_od_short', \n",
    "    'stretch', \n",
    "    'week_day', \n",
    "    'week_num', \n",
    "    'train_key', \n",
    "    'train_departure_date_time', \n",
    "    'train_departure_date_short', \n",
    "    'service_train_departure_date_short', \n",
    "    'operation_date_time', \n",
    "    'operation_date']\n",
    "\n",
    "    return df_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab08a1d6-63fb-4744-b4f6-ed06ff87f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_booking_payment(file_name):\n",
    "\n",
    "    booking_payment_datatype = {\n",
    "        'Booking Code':str,\n",
    "       'Ticket Number':str,\t\n",
    "       'Operation Date':str,\t\n",
    "       'Base Price':float,\n",
    "       'VAT Base Price':float,\n",
    "       'Management Fee':float,\n",
    "       'VAT Management Fee':float,\n",
    "       'Payment Fee':float,\n",
    "       'VAT Payment Fee':float,\n",
    "       'Operation Amount':float,\t\n",
    "       'Penalty Tariff':float,\t\n",
    "       'Compensation Type':str,\t\n",
    "       'Compensation Reason':str,\t\n",
    "       'Compensation Status':str,\n",
    "       'Card Number':str,\n",
    "       'Authorization Code':str,\n",
    "       'Order ID':str,\n",
    "       'Transaction ID':str,\n",
    "       'Status Payment Card':str,\n",
    "       'Card Brand':str,\n",
    "       'Bill Number':str,\n",
    "       'Bill Status':str,\n",
    "       'Train Number':str,\t\n",
    "       'Departure Date':str,\t\n",
    "       'Arrival Date':str,\n",
    "       'OD':str,\n",
    "       'Origin Station':str,\n",
    "       'Destination Station':str,\n",
    "       'Class':str,\n",
    "       'Tariff':str,\t\n",
    "       'Reserved Number of Seats':str,\n",
    "       'Status':str,\n",
    "       'Card Serial Number':str,\n",
    "       'Card User Name':str,\n",
    "       'Sales Station':str,\n",
    "       'Sales Channel':str,\n",
    "       'Sales Equipment Code':str,\n",
    "       'Payment Mode':str,\n",
    "       'Coach Number':str,\t\n",
    "       'Seat Number':str,\n",
    "       'Nationality':str,\n",
    "       'Name':str,\n",
    "       'Surname':str,\n",
    "       'Gender':str,\n",
    "       'Document Type':str,\n",
    "       'Document':str,\n",
    "       'Prefix':str,\n",
    "       'Telephone':str,\n",
    "       'Email':str,\n",
    "       'Profile':str,\t\n",
    "       'Validation Time':str,\n",
    "       'Checked On Board':str,\t\n",
    "       'Detail Type':str,\n",
    "       'Tipology':str,\n",
    "       'Last Operation Channel':str,\n",
    "       'Last Operation Equipment Code':str\n",
    "    }\n",
    "\n",
    "    # read\n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=7, dtype=booking_payment_datatype)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "\n",
    "    # Remove the last row\n",
    "    df_file = df_file.drop(df_file.index[-1])\n",
    "    \n",
    "    # check duplicates\n",
    "    #duplicates = df_file[\"Ticket Number\"].duplicated(keep='first')\n",
    "    #if(duplicates.sum() > 0):\n",
    "    #    prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "    #    df_file.drop_duplicates(subset='Ticket Number', keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    # format the dates\n",
    "    df_file['Operation Date'] = pd.to_datetime(df_file['Operation Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_file['Departure Date'] = pd.to_datetime(df_file['Departure Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    df_file['Arrival Date'] = pd.to_datetime(df_file['Arrival Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # set column names\n",
    "    df_file.columns = [\n",
    "        'booking_code', \n",
    "        'ticket_number',\n",
    "        'operation_date_time', \n",
    "        'base_price', \n",
    "        'base_price_vat', \n",
    "        'management_fee', \n",
    "        'management_fee_vat', \n",
    "        'payment_fee', \n",
    "        'payment_fee_vat', \n",
    "        'operation_amount', \n",
    "        'penalty_tariff', \n",
    "        'amount_not_refunded', \n",
    "        'compensation_type', \n",
    "        'compensation_reason', \n",
    "        'compensation_status', \n",
    "        'card_number', \n",
    "        'authorization_code', \n",
    "        'order_id', \n",
    "        'transaction_id', \n",
    "        'status_payment_card', \n",
    "        'card_brand', \n",
    "        'bill_number', \n",
    "        'bill_status', \n",
    "        'train_number', \n",
    "        'departure_date_time', \n",
    "        'arrival_date_time', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'class', \n",
    "        'tariff', \n",
    "        'reserved_number_of_seats', \n",
    "        'status', \n",
    "        'card_serial_number', \n",
    "        'card_user_name', \n",
    "        'sales_station', \n",
    "        'sales_channel', \n",
    "        'equipment_code', \n",
    "        'payment_mode', \n",
    "        'coach_number', \n",
    "        'seat_number', \n",
    "        'country_code', \n",
    "        'name', \n",
    "        'surname', \n",
    "        'gender', \n",
    "        'document_type', \n",
    "        'document', \n",
    "        'prefix', \n",
    "        'telephone', \n",
    "        'email', \n",
    "        'profile', \n",
    "        'validating_time', \n",
    "        'checked_on_board', \n",
    "        'detail_type', \n",
    "        'tipology', \n",
    "        #'compensated', \n",
    "        #'include_fare_revenue', \n",
    "        'last_operation_channel', \n",
    "        'last_operation_equipment_code'\n",
    "    ]\n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da4684c4-cb9e-4a89-9ffc-9843f34cb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_occupancy(file_name):    \n",
    "    \n",
    "    # define the datatype\n",
    "    occupancy_datatype = {\n",
    "        'Date':str,\n",
    "        'OD':str,\n",
    "        'Origin Station':str,\n",
    "        'Destination Station':str,\n",
    "        'Train ID':str,\n",
    "        'Train Number':str,\n",
    "        'Class':str,\n",
    "        'Total Seats (Quota + Carer + PRM)':str,\n",
    "        'Quota Configuration':str,\n",
    "        'Total Locks (Quota + Carer + PRM)':str,\n",
    "        'For Sale':str,\n",
    "        'Reserved Usual Seats':str,\n",
    "        'Reserved PRM Seats':str,\n",
    "        'Reserved Carer Seats':str,\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)':str,\n",
    "        'Reserved & Lock Usual Seats':str,\n",
    "        'Reserved & Lock PRM Seats':str,\n",
    "        'Reserved & Lock Carer Seats':str,\t\n",
    "        'Total Available':str,\n",
    "        'Validating':str,\n",
    "        'No Show':str,\n",
    "        'UnBooked':str,\t\n",
    "        'Passengers Inc. Infants':str,\n",
    "        'Checked On Board':str\n",
    "    }\n",
    "    \n",
    "    # read the file\n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=6, dtype=occupancy_datatype, parse_dates=['Date'], date_format={'Date':'%Y-%m-%d %H:%M:%S'})\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "    \n",
    "    # Remove the last two row\n",
    "    df_file.drop(df_file.index[-1], inplace=True)\n",
    "    df_file.drop(df_file.index[-1], inplace=True)\n",
    "\n",
    "    # sort before checking for duplicates\n",
    "    df_file.sort_values(by=['Ticket Reserved (Usual + Carer + PRM)', 'Quota Configuration'], ascending=False, inplace=True)\n",
    "\n",
    "    # check duplicates\n",
    "    duplicates = df_file.duplicated(subset=['Date', 'OD', 'Train Number', 'Class'], keep='first')\n",
    "    if(duplicates.sum() > 0):\n",
    "        prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "        df_file.drop_duplicates(subset=['Date', 'OD','Train Number', 'Class'], keep='first', inplace=True, ignore_index=True)\n",
    "        \n",
    "    # reformat date columns\n",
    "    date_time = pd.to_datetime(df_file['Date'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df_file['Date'] = date_time.dt.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    # create the extra columns\n",
    "    df_file['Data_Date'] = datetime.date.today()\n",
    "    df_file['train_key'] = date_time.dt.strftime('%Y-%m-%d') + \" - \" + df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "    \n",
    "    # rename the columns\n",
    "    df_file.columns = [\n",
    "        'date', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'train_id', \n",
    "        'train_number', \n",
    "        'class', \n",
    "        'total_seats', \n",
    "        'quota_configuration', \n",
    "        'total_locks', \n",
    "        'for_sale', \n",
    "        'reserved_usual_seats', \n",
    "        'reserved_prm_seats', \n",
    "        'reserved_carer_seats', \n",
    "        'ticket_reserved', \n",
    "        'reserved_lock_usual_seats', \n",
    "        'reserved_lock_prm_seats', \n",
    "        'reserved_lock_carer_seats', \n",
    "        'total_available', \n",
    "        'validating', \n",
    "        'no_show', \n",
    "        'unbooked', \n",
    "        'passengers_inc_infant', \n",
    "        'checked_on_board', \n",
    "        'data_date', \n",
    "        'train_key'\n",
    "    ]\n",
    "    \n",
    "\n",
    "        \n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52f419de-7c5c-4ace-9685-ed0391bd58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_train_list(df_file, alchemyEngine):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    unique_dates = df_file['departure_date_short'].unique()\n",
    "    date_conditions = ', '.join([f\"'{date}'\" for date in unique_dates])\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = \"train_list_test\"\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        conn.autocommit = True\n",
    "        for date, group in df_file.groupby('departure_date_short'):\n",
    "            # Delete existing records for the current date\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE departure_date_short = \\'{date}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                #prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous data for date {date}: {e}\")\n",
    "                prt_info(\"Trying to insert anyway...\")\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            try:\n",
    "                num_rows = group.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a140b83-233b-4805-ac86-488fc1bc1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_booking_payment(df_file, alchemyEngine):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['operation_date_time'], format=\"%Y-%m-%d %H:%M\").dt.date\n",
    "    unique_dates = dates.unique()\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = \"booking_payment_detailed_test\"\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        conn.autocommit = True\n",
    "\n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Delete existing records for the current date\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(operation_date_time, 'yyyy-mm-dd') = \\'{date}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                #prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous data for date {date}: {e}\")\n",
    "                prt_info(\"Trying to insert anyway...\")\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            try:\n",
    "                num_rows = group.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a690c858-dae7-4111-b639-7f4290fcc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_occupancy(df_file, alchemyEngine):\n",
    "  \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['date']).dt.strftime('%Y-%m-%d')\n",
    "    unique_dates = dates.sort_values().unique()\n",
    "    today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = \"occupancy_list_hist_test\"\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        conn.autocommit = True\n",
    "    \n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Delete existing records for the current date\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(date, 'yyyy-mm-dd') = \\'{date}\\' and data_date = \\'{today}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                #prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous data for date {date}: {e}\")\n",
    "                prt_info(\"Trying to insert anyway...\")\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            try:\n",
    "                group.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3216f5e-a3f2-41b5-a793-1b6ae279a5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-12 11:02:10] Found excel file P05_OC_2024-08-04_to_2024-08-13_@_2024_08_12.nopag.xlsx\n",
      "[2024-08-12 11:02:10] Detected as Occupancy.\n",
      "[2024-08-12 11:02:10] Reading...\n",
      "[2024-08-12 11:02:12] Deleting 54 duplicated entries.\n",
      "[2024-08-12 11:02:13] File read.\n",
      "[2024-08-12 11:02:13] Exporting File...\n",
      "[2024-08-12 11:02:13] Data for 2024-08-04 inserted successfully (544 inserted).\n",
      "[2024-08-12 11:02:13] Data for 2024-08-05 inserted successfully (544 inserted).\n",
      "[2024-08-12 11:02:13] Data for 2024-08-06 inserted successfully (544 inserted).\n",
      "[2024-08-12 11:02:13] Data for 2024-08-07 inserted successfully (552 inserted).\n",
      "[2024-08-12 11:02:14] Data for 2024-08-08 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:14] Data for 2024-08-09 inserted successfully (524 inserted).\n",
      "[2024-08-12 11:02:14] Data for 2024-08-10 inserted successfully (538 inserted).\n",
      "[2024-08-12 11:02:14] Data for 2024-08-11 inserted successfully (544 inserted).\n",
      "[2024-08-12 11:02:15] Data for 2024-08-12 inserted successfully (544 inserted).\n",
      "[2024-08-12 11:02:15] Data for 2024-08-13 inserted successfully (544 inserted).\n",
      "[2024-08-12 11:02:15] File P05_OC_2024-08-04_to_2024-08-13_@_2024_08_12.nopag.xlsx exported successfully.\n",
      "[2024-08-12 11:02:15] Found excel file P05_OC_2024-08-14_to_2024-08-23_@_2024_08_12.nopag.xlsx\n",
      "[2024-08-12 11:02:15] Detected as Occupancy.\n",
      "[2024-08-12 11:02:15] Reading...\n",
      "[2024-08-12 11:02:17] Deleting 48 duplicated entries.\n",
      "[2024-08-12 11:02:17] File read.\n",
      "[2024-08-12 11:02:17] Exporting File...\n",
      "[2024-08-12 11:02:18] Data for 2024-08-14 inserted successfully (552 inserted).\n",
      "[2024-08-12 11:02:18] Data for 2024-08-15 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:18] Data for 2024-08-16 inserted successfully (524 inserted).\n",
      "[2024-08-12 11:02:18] Data for 2024-08-17 inserted successfully (538 inserted).\n",
      "[2024-08-12 11:02:18] Data for 2024-08-18 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:18] Data for 2024-08-19 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:19] Data for 2024-08-20 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:19] Data for 2024-08-21 inserted successfully (564 inserted).\n",
      "[2024-08-12 11:02:19] Data for 2024-08-22 inserted successfully (580 inserted).\n",
      "[2024-08-12 11:02:19] Data for 2024-08-23 inserted successfully (530 inserted).\n",
      "[2024-08-12 11:02:19] Data for 2024-08-24 inserted successfully (2 inserted).\n",
      "[2024-08-12 11:02:19] File P05_OC_2024-08-14_to_2024-08-23_@_2024_08_12.nopag.xlsx exported successfully.\n",
      "[2024-08-12 11:02:19] Found excel file P05_OC_2024-08-24_to_2024-09-02_@_2024_08_12.nopag.xlsx\n",
      "[2024-08-12 11:02:19] Detected as Occupancy.\n",
      "[2024-08-12 11:02:19] Reading...\n",
      "[2024-08-12 11:02:21] Deleting 42 duplicated entries.\n",
      "[2024-08-12 11:02:22] File read.\n",
      "[2024-08-12 11:02:22] Exporting File...\n",
      "[2024-08-12 11:02:22] Data for 2024-08-24 inserted successfully (550 inserted).\n",
      "[2024-08-12 11:02:22] Data for 2024-08-25 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:23] Data for 2024-08-26 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:23] Data for 2024-08-27 inserted successfully (556 inserted).\n",
      "[2024-08-12 11:02:23] Data for 2024-08-28 inserted successfully (564 inserted).\n",
      "[2024-08-12 11:02:23] Data for 2024-08-29 inserted successfully (580 inserted).\n",
      "[2024-08-12 11:02:23] Data for 2024-08-30 inserted successfully (530 inserted).\n",
      "[2024-08-12 11:02:23] Data for 2024-08-31 inserted successfully (550 inserted).\n",
      "[2024-08-12 11:02:24] Data for 2024-09-01 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:24] Data for 2024-09-02 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:24] File P05_OC_2024-08-24_to_2024-09-02_@_2024_08_12.nopag.xlsx exported successfully.\n",
      "[2024-08-12 11:02:24] Found excel file P05_OC_2024-09-03_to_2024-09-12_@_2024_08_12.nopag.xlsx\n",
      "[2024-08-12 11:02:24] Detected as Occupancy.\n",
      "[2024-08-12 11:02:24] Reading...\n",
      "[2024-08-12 11:02:27] Deleting 16 duplicated entries.\n",
      "[2024-08-12 11:02:27] File read.\n",
      "[2024-08-12 11:02:27] Exporting File...\n",
      "[2024-08-12 11:02:27] Data for 2024-09-03 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:27] Data for 2024-09-04 inserted successfully (456 inserted).\n",
      "[2024-08-12 11:02:27] Data for 2024-09-05 inserted successfully (488 inserted).\n",
      "[2024-08-12 11:02:28] Data for 2024-09-06 inserted successfully (426 inserted).\n",
      "[2024-08-12 11:02:28] Data for 2024-09-07 inserted successfully (470 inserted).\n",
      "[2024-08-12 11:02:28] Data for 2024-09-08 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:29] Data for 2024-09-09 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:29] Data for 2024-09-10 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:29] Data for 2024-09-11 inserted successfully (456 inserted).\n",
      "[2024-08-12 11:02:29] Data for 2024-09-12 inserted successfully (488 inserted).\n",
      "[2024-08-12 11:02:29] Data for 2024-09-13 inserted successfully (6 inserted).\n",
      "[2024-08-12 11:02:29] File P05_OC_2024-09-03_to_2024-09-12_@_2024_08_12.nopag.xlsx exported successfully.\n",
      "[2024-08-12 11:02:29] Found excel file P05_OC_2024-09-13_to_2024-09-22_@_2024_08_12.nopag.xlsx\n",
      "[2024-08-12 11:02:29] Detected as Occupancy.\n",
      "[2024-08-12 11:02:29] Reading...\n",
      "[2024-08-12 11:02:32] Deleting 10 duplicated entries.\n",
      "[2024-08-12 11:02:32] File read.\n",
      "[2024-08-12 11:02:32] Exporting File...\n",
      "[2024-08-12 11:02:33] Data for 2024-09-13 inserted successfully (426 inserted).\n",
      "[2024-08-12 11:02:33] Data for 2024-09-14 inserted successfully (470 inserted).\n",
      "[2024-08-12 11:02:33] Data for 2024-09-15 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:33] Data for 2024-09-16 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:34] Data for 2024-09-17 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:34] Data for 2024-09-18 inserted successfully (456 inserted).\n",
      "[2024-08-12 11:02:34] Data for 2024-09-19 inserted successfully (488 inserted).\n",
      "[2024-08-12 11:02:34] Data for 2024-09-20 inserted successfully (426 inserted).\n",
      "[2024-08-12 11:02:35] Data for 2024-09-21 inserted successfully (470 inserted).\n",
      "[2024-08-12 11:02:35] Data for 2024-09-22 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:35] File P05_OC_2024-09-13_to_2024-09-22_@_2024_08_12.nopag.xlsx exported successfully.\n",
      "[2024-08-12 11:02:35] Found excel file P05_OC_2024-09-23_to_2024-09-30_@_2024_08_12.nopag.xlsx\n",
      "[2024-08-12 11:02:35] Detected as Occupancy.\n",
      "[2024-08-12 11:02:35] Reading...\n",
      "[2024-08-12 11:02:37] Deleting 8 duplicated entries.\n",
      "[2024-08-12 11:02:37] File read.\n",
      "[2024-08-12 11:02:37] Exporting File...\n",
      "[2024-08-12 11:02:37] Data for 2024-09-23 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:37] Data for 2024-09-24 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:38] Data for 2024-09-25 inserted successfully (456 inserted).\n",
      "[2024-08-12 11:02:38] Data for 2024-09-26 inserted successfully (488 inserted).\n",
      "[2024-08-12 11:02:38] Data for 2024-09-27 inserted successfully (426 inserted).\n",
      "[2024-08-12 11:02:38] Data for 2024-09-28 inserted successfully (470 inserted).\n",
      "[2024-08-12 11:02:38] Data for 2024-09-29 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:38] Data for 2024-09-30 inserted successfully (448 inserted).\n",
      "[2024-08-12 11:02:38] File P05_OC_2024-09-23_to_2024-09-30_@_2024_08_12.nopag.xlsx exported successfully.\n",
      "[2024-08-12 11:02:38] Exportation finished.\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# get all xlsx files\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        prt_info(f\"Found excel file {file}\")\n",
    "        # check if it is a report\n",
    "        kind_file = get_report_name(file)\n",
    "        \n",
    "        if(kind_file != NO_REPORT):\n",
    "            # get the name\n",
    "            if(kind_file == TRAIN_LIST_REPORT): kind_file_name = 'Train List'\n",
    "            elif(kind_file == OCCUPANCY_REPORT): kind_file_name = 'Occupancy'\n",
    "            elif(kind_file == BOOKING_PAYMENT_REPORT): kind_file_name = 'Booking Payment Detailed'\n",
    "            else: kind_file_name = 'Unknown'\n",
    "            prt_info(f\"Detected as {kind_file_name}.\")\n",
    "\n",
    "            # reading\n",
    "            prt_info(\"Reading...\")\n",
    "            try:\n",
    "                if(kind_file == TRAIN_LIST_REPORT):\n",
    "                    df = read_train_list(file, alchemyEngine)\n",
    "                    prt_info(\"File read.\")\n",
    "                elif(kind_file == BOOKING_PAYMENT_REPORT):\n",
    "                    df = read_booking_payment(file)\n",
    "                    prt_info(\"File read.\")\n",
    "                elif(kind_file == OCCUPANCY_REPORT):\n",
    "                    df = read_occupancy(file)\n",
    "                    prt_info(\"File read.\")\n",
    "                                \n",
    "                else:\n",
    "                    prt_info(f\"Reading of files {kind_file_name} have not been implemented yet.\")\n",
    "            except Exception as e:\n",
    "                prt_info(e)\n",
    "                prt_info(\"Reading failed. Exportation of the file aborted.\")\n",
    "                continue\n",
    "            \n",
    "        \n",
    "            # export the valid files\n",
    "            prt_info(\"Exporting File...\")\n",
    "            try:\n",
    "                if(kind_file == TRAIN_LIST_REPORT):\n",
    "                    export_train_list(df, alchemyEngine)\n",
    "                    prt_info(f\"File {file} exported successfully.\")\n",
    "                elif(kind_file == BOOKING_PAYMENT_REPORT):\n",
    "                    export_booking_payment(df, alchemyEngine)\n",
    "                    prt_info(f\"File {file} exported successfully.\")\n",
    "                elif(kind_file == OCCUPANCY_REPORT):\n",
    "                    export_occupancy(df, alchemyEngine)\n",
    "                    prt_info(f\"File {file} exported successfully.\")\n",
    "                else:\n",
    "                    prt_info(f\"Exportation of files {kind_file_name} have not been implemented yet.\")\n",
    "            except Exception as e:\n",
    "                prt_info(e)\n",
    "                prt_info(\"Exportation failed. Exportation of the file aborted.\")\n",
    "                continue\n",
    "\n",
    "        else: prt_info(\"No report detected on this file.\")\n",
    "prt_info(\"Exportation finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
