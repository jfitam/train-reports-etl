{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70d7d56-b9c1-4f36-bacb-cacaa0828e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V 0.1 \n",
    "# basic functionalities. read and export Train List, Occupancy, and Booking Payment Detailed\n",
    "# \n",
    "# V 0.2 \n",
    "# handle all the information of each kind of report together. \n",
    "#\n",
    "# v 0.3\n",
    "# add general logging\n",
    "# delete all the information in the same query\n",
    "# added importation by chunks\n",
    "# added information of the process of each day\n",
    "#\n",
    "# v 0.4\n",
    "# correct a bug to insert properly insertion entry to audit table\n",
    "# add an error log and alert window to alert for errors\n",
    "# categorize all the message printed by their kind\n",
    "# \n",
    "# v 0.41\n",
    "# fixed bug missing some logging level information during exportation\n",
    "# added message when deleting the day\n",
    "# fixed minor bug regarding the number of entries inserted that are shown in screen\n",
    "#\n",
    "# v 0.5\n",
    "# extend the number of rows to be checked for the header to 50\n",
    "# delete incorrect rows at the end\n",
    "# fixed minor bug with the exporting\n",
    "# read all the sheets of an excel file\n",
    "# improve the detection of the wrong lines at the end. saves them in file\n",
    "# compress result as zip\n",
    "# unify timestamps\n",
    "# arrange output in folders\n",
    "# save duplicates in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23eb465f-e7a6-4825-ac11-7b330b4ccf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection to the database\n",
    "# import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine(\"postgresql+psycopg2://postgres:Renfe2022@172.19.28.174:5433/SalesSystem\", pool_recycle=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39a1be9b-b12b-4563-93d1-245c0a6902bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "# import numpy as np\n",
    "from numpy import sort as np_sort\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import logging\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import shutil\n",
    "\n",
    "# tkinter\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# STATES\n",
    "NO_REPORT = 0\n",
    "TRAIN_LIST_REPORT = 1\n",
    "OCCUPANCY_REPORT = 2\n",
    "BOOKING_PAYMENT_REPORT = 3\n",
    "\n",
    "# ERRORS FOUND\n",
    "errors_found = False\n",
    "\n",
    "# timestamp for all the records\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# folders\n",
    "log_folder = \"log\"\n",
    "export_folder = \"export\"\n",
    "data_folder = \"data\"\n",
    "\n",
    "# tables\n",
    "train_list_table = 'train_list'\n",
    "occupancy_table = 'occupancy_list_hist'\n",
    "bpd_table = 'booking_payment_detailed'\n",
    "\n",
    "#train_list_table = 'train_list_test'\n",
    "#occupancy_table = 'occupancy_list_hist_test'\n",
    "#bpd_table = 'booking_payment_detailed_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c648c4-f9d6-412a-a237-467688f3c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prt_info(string, kind=logging.INFO, nl=True):\n",
    "    global errors_found\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Get the current time\n",
    "    info = f\"[{current_time}] {string}\"\n",
    "\n",
    "    info = '\\r'+info\n",
    "\n",
    "    # log the message in the different loggers\n",
    "    log.log(kind, info)\n",
    "    \n",
    "    # record there is error or warning to set the alert window at the end\n",
    "    if kind >= logging.WARNING:\n",
    "        error_log.log(kind, info)\n",
    "        errors_found = True\n",
    "\n",
    "    # Print the information\n",
    "    print(info, end='' if not nl else '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff81a607-e938-4a64-96ac-eb017940aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "log_name = 'exportation_' + current_time + '.log'\n",
    "\n",
    "# create the directory if not exist\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "log = logging.getLogger(\"log_general\")\n",
    "log.setLevel(logging.INFO) \n",
    "general_handler = logging.FileHandler(f\"{log_folder}/{log_name}\", mode='w')\n",
    "general_handler.setLevel(logging.INFO)\n",
    "log.addHandler(general_handler)\n",
    "\n",
    "error_log = logging.getLogger(\"log_error\")\n",
    "error_handler = logging.FileHandler(f\"{log_folder}/error_{log_name}\", mode='w')\n",
    "error_handler.setLevel(logging.WARNING)\n",
    "error_log.addHandler(error_handler)\n",
    "\n",
    "#debug_log = logging.getLogger(\"log_debug\")\n",
    "#debug_handler = logging.FileHandler(\"debug_\" + log_name, mode='w')\n",
    "#debug_handler.setLevel(logging.DEBUG)\n",
    "# debug_log.addHandler(debug_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "605a995c-d1f1-4539-87d6-94396745528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version control\n",
    "version = 0.5\n",
    "max_control = 0\n",
    "\n",
    "# check if the current version is the last one\n",
    "query = \"SELECT version FROM \\\"AFC\\\".exporter_version_control\"\n",
    "all_versions = pd.read_sql_query(query, alchemyEngine)\n",
    "max_version = all_versions['version'].max()\n",
    "\n",
    "try:\n",
    "    if version > max_version:\n",
    "        # add the new version\n",
    "        with alchemyEngine.connect() as conn:\n",
    "            query = text(f\"insert into \\\"AFC\\\".exporter_version_control(date, version) values (\\'{datetime.datetime.now()}\\',\\'{version}\\')\")\n",
    "            conn.execute(query)\n",
    "            conn.commit()\n",
    "        \n",
    "    elif version < max_version:\n",
    "        # this program is out of date, terminating execution\n",
    "        prt_info(\"Current exporter is out of date. Please, use the last version to export data.\", logging.ERROR)\n",
    "        sys.exit()\n",
    "\n",
    "    #else:\n",
    "        #current is the last version. Do nothing\n",
    "\n",
    "except Exception as e:\n",
    "    prt_info(\"The version could not be checked in the database\", logging.ERROR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a3f3f76-9e01-4666-91c6-45ac0eb035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that detects which kind of report is the excel file\n",
    "def get_report_name(excel_file_path, sheet=0):\n",
    "\n",
    "    train_list_header = pd.DataFrame([\n",
    "        'Departure Date',\n",
    "        'Train Number',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Coach Number',\n",
    "        'Seat Number',\n",
    "        'Class',\n",
    "        'Booking Code',\n",
    "        'Ticket Number',\n",
    "        'Tariff',\n",
    "        'Status',\n",
    "        'Payment Mode',\n",
    "        'Media Type',\n",
    "        'Sales Channel',\n",
    "        'Base Price',\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff',\n",
    "        'Amount Not Refunded',\n",
    "        'Compensation Type',\n",
    "        'Compensation Reason',\n",
    "        'Compensation Status',\n",
    "        'Nationality',\n",
    "        'Gender',\n",
    "        'Name',\n",
    "        'Surname',\n",
    "        'Document',\n",
    "        'Prefix',\n",
    "        'Telephone',\n",
    "        'Profile',\n",
    "        'Special Needs',\t\n",
    "        'Validation Time',\n",
    "        'Group',\n",
    "        'Checked On Board',\n",
    "        'Last Operation Channel',\n",
    "        'Last Operation Equipment Code'\n",
    "        ])\n",
    "    \n",
    "    occupancy_header = pd.DataFrame([\n",
    "        'Date',\n",
    "        'OD',\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Train ID',\n",
    "        'Train Number',\n",
    "        'Class',\n",
    "        'Total Seats (Quota + Carer + PRM)',\n",
    "        'Quota Configuration',\n",
    "        'Total Locks (Quota + Carer + PRM)',\n",
    "        'For Sale',\n",
    "        'Reserved Usual Seats',\n",
    "        'Reserved PRM Seats',\n",
    "        'Reserved Carer Seats',\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)',\n",
    "        'Reserved & Lock Usual Seats',\n",
    "        'Reserved & Lock PRM Seats',\n",
    "        'Reserved & Lock Carer Seats',\t\n",
    "        'Total Available',\n",
    "        'Validating',\n",
    "        'No Show',\n",
    "        'UnBooked',\t\n",
    "        'Passengers Inc. Infants',\n",
    "        'Checked On Board'\n",
    "    ])\n",
    "    \n",
    "    bpd_header = pd.DataFrame([\n",
    "       'Booking Code',\n",
    "       'Ticket Number',\t\n",
    "       'Operation Date',\t\n",
    "       'Base Price',\n",
    "       'VAT Base Price',\n",
    "       'Management Fee',\n",
    "       'VAT Management Fee',\n",
    "       'Payment Fee',\n",
    "       'VAT Payment Fee',\n",
    "       'Operation Amount',\t\n",
    "       'Penalty Tariff',\t\n",
    "       'Compensation Type',\t\n",
    "       'Compensation Reason',\t\n",
    "       'Compensation Status',\n",
    "       'Card Number',\n",
    "       'Authorization Code',\n",
    "       'Order ID',\n",
    "       'Transaction ID',\n",
    "       'Status Payment Card',\n",
    "       'Card Brand',\n",
    "       'Bill Number',\n",
    "       'Bill Status',\n",
    "       'Train Number',\t\n",
    "       'Departure Date',\t\n",
    "       'Arrival Date',\n",
    "       'OD',\n",
    "       'Origin Station',\n",
    "       'Destination Station',\n",
    "       'Class',\n",
    "       'Tariff',\t\n",
    "       'Reserved Number of Seats',\n",
    "       'Status',\n",
    "       'Card Serial Number',\n",
    "       'Card User Name',\n",
    "       'Sales Station',\n",
    "       'Sales Channel',\n",
    "       'Sales Equipment Code',\n",
    "       'Payment Mode',\n",
    "       'Coach Number',\t\n",
    "       'Seat Number',\n",
    "       'Nationality',\n",
    "       'Name',\n",
    "       'Surname',\n",
    "       'Gender',\n",
    "       'Document Type',\n",
    "       'Document',\n",
    "       'Prefix',\n",
    "       'Telephone',\n",
    "       'Email',\n",
    "       'Profile',\t\n",
    "       'Validation Time',\n",
    "       'Checked On Board',\t\n",
    "       'Detail Type',\n",
    "       'Tipology',\n",
    "       'Last Operation Channel',\n",
    "       'Last Operation Equipment Code'\n",
    "    \n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # read the header of the file\n",
    "        file_header = pd.read_excel(excel_file_path, sheet_name=sheet, nrows=50, header=None)\n",
    "    except Exception as e:\n",
    "        prt_info(f\"There is a problem reading the file: {e}\", kind=logging.ERROR)\n",
    "        return 0, NO_REPORT\n",
    "    \n",
    "    # go through the read part of the file\n",
    "    for index, row in enumerate(file_header.values):\n",
    "        # clean\n",
    "        row = row[pd.notnull(row)]\n",
    "        row = pd.DataFrame(row)\n",
    "        \n",
    "        # comparision\n",
    "        if(row.equals(train_list_header)): return index + 1, TRAIN_LIST_REPORT\n",
    "        elif(row.equals(occupancy_header)): return index + 1, OCCUPANCY_REPORT\n",
    "        elif(row.equals(bpd_header)): return index + 1, BOOKING_PAYMENT_REPORT\n",
    "            \n",
    "    # no report found\n",
    "    return 0, NO_REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b18b48-847c-4889-9e93-a112717e12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_list(file_name, alchemyEngine, sheet=0):\n",
    "\n",
    "    global current_time\n",
    "    \n",
    "    # train_list datatype\n",
    "    train_list_datatype = {\n",
    "    'Departure Date': str,\n",
    "    'Train Number': str,\n",
    "    'OD': str,\n",
    "    'Origin Station': str,\n",
    "    'Destination Station': str,\n",
    "    'Coach Number': str,\n",
    "    'Seat Number': str,\n",
    "    'Class': str,\n",
    "    'Booking Code': str,\n",
    "    'Ticket Number': str,\n",
    "    'Tariff': str,\n",
    "    'Status': str,\n",
    "    'Payment Mode': str,\n",
    "    'Media Type': str,\n",
    "    'Sales Channel': str,\n",
    "    'Base Price': float,\n",
    "    'VAT Base Price': float,\n",
    "    'Management Fee': float,\n",
    "    'VAT Management Fee': float,\n",
    "    'Payment Fee': float,\n",
    "    'VAT Payment Fee': float,\n",
    "    'Operation Amount':\tfloat,\n",
    "    'Penalty Tariff': float,\n",
    "    'Amount Not Refunded': float,\n",
    "    'Compensation Type': str,\n",
    "    'Compensation Reason': str,\n",
    "    'Compensation Status': str,\n",
    "    'Nationality': str,\n",
    "    'Gender': str,\n",
    "    'Name': str,\n",
    "    'Surname': str,\n",
    "    'Document': str,\n",
    "    'Prefix': str,\n",
    "    'Telephone': str,\n",
    "    'Profile': str,\n",
    "    'Special Needs': str,\t\n",
    "    'Validation Time': str,\n",
    "    'Group': str,\n",
    "    'Checked On Board': str,\n",
    "    'Last Operation Channel': str,\n",
    "    'Last Operation Equipment Code': str\n",
    "    }\n",
    "    \n",
    "    # get the first line of the report\n",
    "    first_row, name_report = get_report_name(file_name, sheet)\n",
    "    \n",
    "    if (name_report != TRAIN_LIST_REPORT):\n",
    "        raise Exception(f\"Wrong function invoked for sheet '{sheet}' from '{file_name}'\")\n",
    "        \n",
    "    #read the file\n",
    "    try:\n",
    "        # open file\n",
    "        df_file = pd.read_excel(file_name, header=0, sheet_name=sheet, skiprows=(first_row-1), dtype=str\n",
    "                                #,                                 \n",
    "                                #parse_dates=['Departure Date', 'Validation Time'],\n",
    "                                #date_format={'Departure Date': '%Y-%m-%d %H:%M', 'Validation Time': '%Y-%m-%d  %H:%M'}\n",
    "                               )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "\n",
    "    #delete empty columns\n",
    "    df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # format date columns\n",
    "    date_cols = []\n",
    "    for col in date_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "    # format datetime columns\n",
    "    datetime_cols = ['Departure Date','Validation Time']\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # format numeric columns\n",
    "    num_cols = [\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff',\n",
    "        'Amount Not Refunded']\n",
    "    \n",
    "    for col in num_cols:\n",
    "        df_file[col] = pd.to_numeric(df_file[col], errors='coerce')\n",
    "    \n",
    "    # turn nullable columns to empty space\n",
    "    nullable_cols = ['Validation Time', \n",
    "                     'Special Needs', \n",
    "                     'Payment Mode', \n",
    "                     'Media Type',\n",
    "                    'Penalty Tariff',\n",
    "                    'Amount Not Refunded',\n",
    "                    'Compensation Type',\n",
    "                    'Compensation Reason',\n",
    "                    'Compensation Status',\n",
    "                    'Nationality',\n",
    "                    'Special Needs',\n",
    "                    'Last Operation Equipment Code',\n",
    "                    'Group',\n",
    "                    'Checked On Board']\n",
    "    \n",
    "    # check wrong lines. delete all nan or nat and save them in a separate file\n",
    "    df_nan = df_file.dropna(inplace=True, how='any', subset=df_file.columns.difference(nullable_cols))\n",
    "    if df_nan is not None:\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df_nan.to_csv(f\"{export_folder}/Train List error rows {current_time}.csv.zip\")\n",
    "    \n",
    "    #check if there are still data entries\n",
    "    if df_file.shape[0] == 0:\n",
    "        raise Exception(f\"Empty dataset after cleaning.\")   \n",
    "    \n",
    "    # create extra columns\n",
    "    df_file['Train_hour'] = df_file['Departure Date'].dt.strftime('%H:%M')\n",
    "    df_file['Departure_Date_Short'] = df_file['Departure Date'].dt.strftime('%Y-%m-%d')\n",
    "    df_file['Train-OD Short'] = df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "    df_file['CORRIDOR'] = df_file['Train Number'].str[:2]\n",
    "    df_file['WEEK_DAY'] = df_file['Departure Date'].dt.strftime('%a')\n",
    "    df_file['WEEK_NUM'] = df_file['Departure Date'].dt.isocalendar().week\n",
    "    df_file['train_key'] = df_file['Departure_Date_Short'] + \" - \" + df_file['Train-OD Short']\n",
    "    \n",
    "    \n",
    "    # get the train departure\n",
    "    try:\n",
    "        train_hours = pd.read_sql_table('train_departure_times', alchemyEngine, schema='AFC')\n",
    "    except Exception as e:\n",
    "        raise(f\"Error fetching the departure times from database: {e}\")\n",
    "        \n",
    "    train_hours.columns = ['Train Number', 'train_departure_date_time']\n",
    "    df_file = pd.merge(df_file, train_hours, on=\"Train Number\", how=\"left\")\n",
    "    \n",
    "    #check if there is missing hours for the train numbers of this file\n",
    "    if(df_file['train_departure_date_time'].isnull().sum() > 0):\n",
    "        trains_missing = df_file['Train Number'][df_file['train_departure_date_time'].isnull()].unique()\n",
    "        raise Exception(f\"There are missing departing hours in the database. Please, check the following trains: {', '.join(trains_missing)}\")\n",
    "    \n",
    "    # calculate the departing time of the train\n",
    "    df_file['train_departure_date_time'] = pd.to_datetime(df_file['Departure_Date_Short'].astype(str) + \" \" + df_file['train_departure_date_time'].astype(str))\n",
    "    train_date_adjustment = df_file['train_departure_date_time'].dt.time > df_file['Departure Date'].dt.time\n",
    "    df_file['train_departure_date_time'] = df_file['train_departure_date_time'] - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "    df_file['train_departure_date_short'] = df_file['Departure Date'].dt.date - pd.to_timedelta(train_date_adjustment.astype(int), unit=\"D\")\n",
    "    \n",
    "    # calculate the services date (reduce one day if it is an early train before maintenance window)\n",
    "    service_date_adjustment = df_file['train_departure_date_time'].dt.time <= datetime.time(5, 0)\n",
    "    df_file['Service_Date'] = df_file['train_departure_date_short'] - pd.to_timedelta(service_date_adjustment.astype(int), unit=\"D\")\n",
    "    \n",
    "    # get the date time of the operation\n",
    "    ticket_numbers = \", \".join(f\"'{ticket}'\" for ticket in df_file['Ticket Number'].unique())\n",
    "    query = f\"\"\"\n",
    "    SELECT ticket_number AS \\\"Ticket Number\\\", operation_date_time\n",
    "    FROM \\\"AFC\\\".{bpd_table}\n",
    "    WHERE ticket_number IN ({ticket_numbers})\n",
    "    \"\"\"\n",
    "    df_operation_date_times = pd.read_sql_query(query, alchemyEngine)\n",
    "    df_file = pd.merge(df_file, df_operation_date_times, on=\"Ticket Number\", how=\"left\")\n",
    "    df_file['operation_date'] = pd.to_datetime(df_file['operation_date_time'], errors='coerce', format='%Y-%m-%d %H:%M:%S').dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # transform columns date and datetime to text\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "    for col in date_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # set the headers according to database\n",
    "    df_file.columns = [\n",
    "    'departure_date', \n",
    "    'train_number', \n",
    "    'od', \n",
    "    'origin_station', \n",
    "    'destination_station',\n",
    "    'coach_number', \n",
    "    'seat_number', \n",
    "    'class', \n",
    "    'booking_code', \n",
    "    'ticket_number', \n",
    "    'tariff', \n",
    "    'status', \n",
    "    'payment_mode', \n",
    "    'media_type', \n",
    "    'sales_channel', \n",
    "    'base_price', \n",
    "    'vat_base_price',\n",
    "    'management_fee', \n",
    "    'vat_management_fee', \n",
    "    'payment_fee', \n",
    "    'vat_payment_fee', \n",
    "    'operation_amount', \n",
    "    'penalty_tariff', \n",
    "    'amount_not_refunded', \n",
    "    'compensation_type', \n",
    "    'compensation_reason', \n",
    "    'compensation_status', \n",
    "    'nationality', \n",
    "    'gender', \n",
    "    'name', \n",
    "    'surname', \n",
    "    'document', \n",
    "    'prefix', \n",
    "    'telephone', \n",
    "    'profile', \n",
    "    'special_needs', \n",
    "    'validating_time', \n",
    "    'groupyn', \n",
    "    'checked_on_board', \n",
    "    'last_operation_channel', \n",
    "    'last_operation_equipment_code', \n",
    "    'train_hour', \n",
    "    'departure_date_short', \n",
    "    'train_od_short', \n",
    "    'stretch', \n",
    "    'week_day', \n",
    "    'week_num', \n",
    "    'train_key', \n",
    "    'train_departure_date_time', \n",
    "    'train_departure_date_short', \n",
    "    'service_train_departure_date_short', \n",
    "    'operation_date_time', \n",
    "    'operation_date']\n",
    "\n",
    "    return df_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab08a1d6-63fb-4744-b4f6-ed06ff87f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_booking_payment(file_name, sheet=0):\n",
    "\n",
    "    booking_payment_datatype = {\n",
    "        'Booking Code':str,\n",
    "       'Ticket Number':str,\t\n",
    "       'Operation Date':str,\t\n",
    "       'Base Price':float,\n",
    "       'VAT Base Price':float,\n",
    "       'Management Fee':float,\n",
    "       'VAT Management Fee':float,\n",
    "       'Payment Fee':float,\n",
    "       'VAT Payment Fee':float,\n",
    "       'Operation Amount':float,\t\n",
    "       'Penalty Tariff':float,\n",
    "        #amount no refunded\n",
    "       'Compensation Type':str,\t\n",
    "       'Compensation Reason':str,\t\n",
    "       'Compensation Status':str,\n",
    "       'Card Number':str,\n",
    "       'Authorization Code':str,\n",
    "       'Order ID':str,\n",
    "       'Transaction ID':str,\n",
    "       'Status Payment Card':str,\n",
    "       'Card Brand':str,\n",
    "       'Bill Number':str,\n",
    "       'Bill Status':str,\n",
    "       'Train Number':str,\t\n",
    "       'Departure Date':str,\t\n",
    "       'Arrival Date':str,\n",
    "       'OD':str,\n",
    "       'Origin Station':str,\n",
    "       'Destination Station':str,\n",
    "       'Class':str,\n",
    "       'Tariff':str,\t\n",
    "       'Reserved Number of Seats':str,\n",
    "       'Status':str,\n",
    "       'Card Serial Number':str,\n",
    "       'Card User Name':str,\n",
    "       'Sales Station':str,\n",
    "       'Sales Channel':str,\n",
    "       'Sales Equipment Code':str,\n",
    "       'Payment Mode':str,\n",
    "       'Coach Number':str,\t\n",
    "       'Seat Number':str,\n",
    "       'Nationality':str,\n",
    "       'Name':str,\n",
    "       'Surname':str,\n",
    "       'Gender':str,\n",
    "       'Document Type':str,\n",
    "       'Document':str,\n",
    "       'Prefix':str,\n",
    "       'Telephone':str,\n",
    "       'Email':str,\n",
    "       'Profile':str,\t\n",
    "       'Validation Time':str,\n",
    "       'Checked On Board':str,\t\n",
    "       'Detail Type':str,\n",
    "       'Tipology':str,\n",
    "       'Last Operation Channel':str,\n",
    "       'Last Operation Equipment Code':str\n",
    "    }\n",
    "\n",
    "    # get the first line of the report\n",
    "    first_row, name_report = get_report_name(file_name, sheet)\n",
    "\n",
    "    if (name_report != BOOKING_PAYMENT_REPORT):\n",
    "        raise Exception(f\"Wrong function invoked for sheet '{sheet}' from '{file_name}'\")\n",
    "    \n",
    "    # read\n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, sheet_name=sheet, skiprows=(first_row-1), dtype=str)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "\n",
    "    #delete empty columns\n",
    "    df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # format date columns\n",
    "    date_cols = []\n",
    "    for col in date_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "    # format datetime columns\n",
    "    datetime_cols = ['Operation Date','Departure Date','Arrival Date']\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # format numeric columns\n",
    "    num_cols = [\n",
    "        'Base Price',\n",
    "        'VAT Base Price',\n",
    "        'Management Fee',\n",
    "        'VAT Management Fee',\n",
    "        'Payment Fee',\n",
    "        'VAT Payment Fee',\n",
    "        'Operation Amount',\n",
    "        'Penalty Tariff']\n",
    "    \n",
    "    for col in num_cols:\n",
    "        df_file[col] = pd.to_numeric(df_file[col], errors='coerce')\n",
    "    \n",
    "    # turn nullable columns to empty space\n",
    "    nullable_cols = [\n",
    "       'Compensation Type',\t\n",
    "       'Compensation Reason',\t\n",
    "       'Compensation Status',\n",
    "       'Card Number',\n",
    "       'Authorization Code',\n",
    "       'Order ID',\n",
    "       'Transaction ID',\n",
    "       'Status Payment Card',\n",
    "       'Card Brand',\n",
    "       'Bill Number',\n",
    "       'Bill Status',\n",
    "       'Reserved Number of Seats',\n",
    "       'Card Serial Number',\n",
    "       'Card User Name',\n",
    "       'Sales Station',\n",
    "       'Sales Equipment Code',\n",
    "       'Coach Number',\t\n",
    "       'Seat Number',\n",
    "       'Nationality',\n",
    "       'Name',\n",
    "       'Surname',\n",
    "       'Gender',\n",
    "       'Document Type',\n",
    "       'Document',\n",
    "       'Prefix',\n",
    "       'Telephone',\n",
    "       'Email',\n",
    "       'Profile',\t\n",
    "       'Validation Time',\n",
    "       'Checked On Board',\t\n",
    "       'Detail Type',\n",
    "       'Tipology',\n",
    "       'Last Operation Channel',\n",
    "       'Last Operation Equipment Code']\n",
    "    \n",
    "    # check wrong lines. delete all nan or nat and save them in a separate file\n",
    "    df_nan = df_file.dropna(inplace=True, how='any', subset=df_file.columns.difference(nullable_cols))\n",
    "    if df_nan is not None:\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df_nan.to_csv(f\"{export_folder}/Booking Payment Detailed error rows {current_time}.csv.zip\")\n",
    "    \n",
    "    #check if there are still data entries\n",
    "    if df_file.shape[0] == 0:\n",
    "        raise Exception(f\"Empty dataset after cleaning.\")   \n",
    "\n",
    "    # transform columns date and datetime to text\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "    for col in date_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "    # set column names\n",
    "    df_file.columns = [\n",
    "        'booking_code', \n",
    "        'ticket_number',\n",
    "        'operation_date_time', \n",
    "        'base_price', \n",
    "        'base_price_vat', \n",
    "        'management_fee', \n",
    "        'management_fee_vat', \n",
    "        'payment_fee', \n",
    "        'payment_fee_vat', \n",
    "        'operation_amount', \n",
    "        'penalty_tariff', \n",
    "        #'amount_not_refunded', \n",
    "        'compensation_type', \n",
    "        'compensation_reason', \n",
    "        'compensation_status', \n",
    "        'card_number', \n",
    "        'authorization_code', \n",
    "        'order_id', \n",
    "        'transaction_id', \n",
    "        'status_payment_card', \n",
    "        'card_brand', \n",
    "        'bill_number', \n",
    "        'bill_status', \n",
    "        'train_number', \n",
    "        'departure_date_time', \n",
    "        'arrival_date_time', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'class', \n",
    "        'tariff', \n",
    "        'reserved_number_of_seats', \n",
    "        'status', \n",
    "        'card_serial_number', \n",
    "        'card_user_name', \n",
    "        'sales_station', \n",
    "        'sales_channel', \n",
    "        'equipment_code', \n",
    "        'payment_mode', \n",
    "        'coach_number', \n",
    "        'seat_number', \n",
    "        'country_code', \n",
    "        'name', \n",
    "        'surname', \n",
    "        'gender', \n",
    "        'document_type', \n",
    "        'document', \n",
    "        'prefix', \n",
    "        'telephone', \n",
    "        'email', \n",
    "        'profile', \n",
    "        'validating_time', \n",
    "        'checked_on_board', \n",
    "        'detail_type', \n",
    "        'tipology', \n",
    "        #'compensated', \n",
    "        #'include_fare_revenue', \n",
    "        'last_operation_channel', \n",
    "        'last_operation_equipment_code'\n",
    "    ]\n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da4684c4-cb9e-4a89-9ffc-9843f34cb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_occupancy(file_name, sheet=0):    \n",
    "    \n",
    "    # define the datatype\n",
    "    occupancy_datatype = {\n",
    "        'Date':str,\n",
    "        'OD':str,\n",
    "        'Origin Station':str,\n",
    "        'Destination Station':str,\n",
    "        'Train ID':str,\n",
    "        'Train Number':str,\n",
    "        'Class':str,\n",
    "        'Total Seats (Quota + Carer + PRM)':str,\n",
    "        'Quota Configuration':str,\n",
    "        'Total Locks (Quota + Carer + PRM)':str,\n",
    "        'For Sale':str,\n",
    "        'Reserved Usual Seats':str,\n",
    "        'Reserved PRM Seats':str,\n",
    "        'Reserved Carer Seats':str,\t\n",
    "        'Ticket Reserved (Usual + Carer + PRM)':str,\n",
    "        'Reserved & Lock Usual Seats':str,\n",
    "        'Reserved & Lock PRM Seats':str,\n",
    "        'Reserved & Lock Carer Seats':str,\t\n",
    "        'Total Available':str,\n",
    "        'Validating':str,\n",
    "        'No Show':str,\n",
    "        'UnBooked':str,\t\n",
    "        'Passengers Inc. Infants':str,\n",
    "        'Checked On Board':str\n",
    "    }\n",
    "\n",
    "    # get the first line of the report\n",
    "    first_row, name_report = get_report_name(file_name, sheet)\n",
    "\n",
    "    if (name_report != OCCUPANCY_REPORT):\n",
    "        raise Exception(f\"Wrong function invoked for sheet '{sheet}' from '{file_name}'\")\n",
    "        \n",
    "    try:\n",
    "        df_file = pd.read_excel(file_name, header=0, skiprows=(first_row-1), sheet_name=sheet, dtype=occupancy_datatype, parse_dates=['Date'], date_format={'Date':'%Y-%m-%d %H:%M:%S'})\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error opening the file: {e}\")\n",
    "\n",
    "    #delete empty columns\n",
    "    df_file = df_file.loc[:, ~df_file.columns.str.contains('^Unnamed')]\n",
    "                \n",
    "    # format date columns\n",
    "    date_cols = []\n",
    "    for col in date_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d')\n",
    "    \n",
    "    # format datetime columns\n",
    "    datetime_cols = ['Date']\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = pd.to_datetime(df_file[col], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # format numeric columns\n",
    "    num_cols = []\n",
    "    \n",
    "    for col in num_cols:\n",
    "        df_file[col] = pd.to_numeric(df_file[col], errors='coerce')\n",
    "    \n",
    "    # turn nullable columns to empty space\n",
    "    nullable_cols = [\n",
    "        'Origin Station',\n",
    "        'Destination Station',\n",
    "        'Train ID',\n",
    "        'Total Seats (Quota + Carer + PRM)',\n",
    "        'Total Locks (Quota + Carer + PRM)',\n",
    "        'For Sale',\n",
    "        'Reserved Usual Seats',\n",
    "        'Reserved PRM Seats',\n",
    "        'Reserved Carer Seats',\t\n",
    "        'Reserved & Lock Usual Seats',\n",
    "        'Reserved & Lock PRM Seats',\n",
    "        'Reserved & Lock Carer Seats',\t\n",
    "        'Total Available',\n",
    "        'Validating',\n",
    "        'No Show',\n",
    "        'UnBooked',\t\n",
    "        'Passengers Inc. Infants',\n",
    "        'Checked On Board']\n",
    "    \n",
    "    # check wrong lines. delete all nan or nat and save them in a separate file\n",
    "    df_nan = df_file.dropna(inplace=True, how='any', subset=df_file.columns.difference(nullable_cols))\n",
    "    if df_nan is not None:\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df_nan.to_csv(f\"{export_folder}/Occupancy error rows {current_time}.csv.zip\")\n",
    "    \n",
    "    #check if there are still data entries\n",
    "    if df_file.shape[0] == 0:\n",
    "        raise Exception(f\"Empty dataset after cleaning.\")\n",
    "    \n",
    "    # transform columns date and datetime to text\n",
    "    for col in datetime_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "    for col in date_cols:\n",
    "        df_file[col] = df_file[col].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "    # create the extra columns\n",
    "    df_file['Data_Date'] = datetime.date.today()\n",
    "    df_file['train_key'] = df_file['Date'] + \" - \" + df_file['Train Number'] + \" - \" + df_file['OD']\n",
    "    \n",
    "    # rename the columns\n",
    "    df_file.columns = [\n",
    "        'date', \n",
    "        'od', \n",
    "        'origin_station', \n",
    "        'destination_station', \n",
    "        'train_id', \n",
    "        'train_number', \n",
    "        'class', \n",
    "        'total_seats', \n",
    "        'quota_configuration', \n",
    "        'total_locks', \n",
    "        'for_sale', \n",
    "        'reserved_usual_seats', \n",
    "        'reserved_prm_seats', \n",
    "        'reserved_carer_seats', \n",
    "        'ticket_reserved', \n",
    "        'reserved_lock_usual_seats', \n",
    "        'reserved_lock_prm_seats', \n",
    "        'reserved_lock_carer_seats', \n",
    "        'total_available', \n",
    "        'validating', \n",
    "        'no_show', \n",
    "        'unbooked', \n",
    "        'passengers_inc_infant', \n",
    "        'checked_on_board', \n",
    "        'data_date', \n",
    "        'train_key'\n",
    "    ]\n",
    "      \n",
    "    # return\n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3819a751-b9a7-482d-91be-a9e949d90040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each pair of dates (beginning, end) of the streak days in the submitted list of dates\n",
    "def get_date_pairs(df, column):\n",
    "    try:\n",
    "        date_pairs = []\n",
    "        date_col = np_sort(pd.to_datetime(df[column]).dt.date.unique())\n",
    "        date_begin = date_col.min()\n",
    "        date_end = date_col.min()\n",
    "        day_count = (date_col.max() - date_col.min()).days + 1\n",
    "        \n",
    "        # if there is a single date, return that\n",
    "        if(len(date_col) == 1):\n",
    "            return [[date_col.min(), date_col.max()]]\n",
    "        \n",
    "        # iterate through the dates\n",
    "        for d in date_col:\n",
    "            #skip the first date\n",
    "            if(d == date_col.min()): continue\n",
    "        \n",
    "            # check if it is continous\n",
    "            if((d - date_end).days == 1):\n",
    "                date_end = d\n",
    "            else:\n",
    "                date_pairs.append([date_begin.strftime('%Y-%m-%d'), date_end.strftime('%Y-%m-%d')])\n",
    "                date_begin = d\n",
    "                date_end = d\n",
    "        #at the end insert the last element\n",
    "        if(date_begin is not None):\n",
    "            date_pairs.append([date_begin.strftime('%Y-%m-%d'), date_end.strftime('%Y-%m-%d')])\n",
    "            \n",
    "    \n",
    "        return date_pairs\n",
    "    \n",
    "    except Exception as e: \n",
    "        raise Exception(f\"There was an error while reading the dates of the report: {e}\", logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52f419de-7c5c-4ace-9685-ed0391bd58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_train_list(df_file, alchemyEngine):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    unique_dates = df_file['departure_date_short'].unique()\n",
    "    date_conditions = ', '.join([f\"'{date}'\" for date in unique_dates])\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = train_list_table\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        \n",
    "        # get the dates of the record\n",
    "        date_pairs = get_date_pairs(df_file, 'departure_date_short')\n",
    "\n",
    "        # notice a warning if there are missing dates in the middle of the data\n",
    "        if(len(date_pairs) >1):\n",
    "            prt_info(\"The dates on the report Train List are not consecutive. Make sure all the files of the day has been submitted\", kind=logging.WARNING)\n",
    "        \n",
    "        # delete the previous records\n",
    "        for date_from, date_to in date_pairs:\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE departure_date_short between \\'{date_from}\\' and \\'{date_to}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Previous data from {date_from} to {date_to} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous data from {date_from} to {date_to} {e}\", kind=logging.ERROR)\n",
    "\n",
    "        # insert the data day by day\n",
    "        for date, group in df_file.groupby('departure_date_short'):\n",
    "            # Insert the data for the current date\n",
    "            chunk_size = 500\n",
    "            try:\n",
    "                for chunk in range(0, len(group), chunk_size):\n",
    "                    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                    conn.commit()\n",
    "                    if ((chunk+chunk_size)<len(group)): prt_info(f\"Data for {date}: {chunk+chunk_size} entries inserted.\", kind=logging.DEBUG, nl=False)\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a140b83-233b-4805-ac86-488fc1bc1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_booking_payment(df_file, alchemyEngine):\n",
    "    \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['operation_date_time'], format=\"%Y-%m-%d %H:%M\").dt.date\n",
    "    unique_dates = dates.unique()\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = bpd_table\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        conn.autocommit = True\n",
    "\n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Delete existing records for the current date\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(operation_date_time, 'yyyy-mm-dd') = \\'{date}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Previous data for {date} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous data for date {date}: {e}\", kind=logging.ERROR)\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            chunk_size = 500\n",
    "            try:\n",
    "                for chunk in range(0, len(group), chunk_size):\n",
    "                    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                    conn.commit()\n",
    "                    if ((chunk+chunk_size)<len(group)): prt_info(f\"Data for {date}: {chunk+chunk_size} entries inserted.\", kind=logging.DEBUG, nl=False)\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a690c858-dae7-4111-b639-7f4290fcc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_occupancy(df_file, alchemyEngine):\n",
    "  \n",
    "    # Extract unique dates from the DataFrame\n",
    "    dates = pd.to_datetime(df_file['date']).dt.strftime('%Y-%m-%d')\n",
    "    unique_dates = dates.sort_values().unique()\n",
    "    today = datetime.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # variables of the ddbb\n",
    "    table_name = occupancy_table\n",
    "    db_schema = \"AFC\"\n",
    "    \n",
    "    # Write the DataFrame to the PostgreSQL table\n",
    "    with alchemyEngine.connect() as conn:\n",
    "        # get the dates of the record\n",
    "        date_pairs = get_date_pairs(df_file, 'date')\n",
    "\n",
    "        # notice a warning if there are missing dates in the middle of the data\n",
    "        if(len(date_pairs) >1):\n",
    "            prt_info(\"The dates on the report Occupancy are not consecutive. Make sure all the files of the day has been submitted\", kind=logging.WARNING)\n",
    "        \n",
    "        # delete the previous records\n",
    "        for date_from, date_to in date_pairs:\n",
    "            try:\n",
    "                delete_query = text(f\"DELETE FROM \\\"{db_schema}\\\".{table_name} WHERE to_char(date, 'yyyy-mm-dd') between \\'{date_from}\\' and \\'{date_to}\\'and data_date = \\'{today}\\'\")\n",
    "                conn.execute(delete_query)\n",
    "                conn.commit()\n",
    "                prt_info(f\"Previous data from {date_from} to {date_to} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                prt_info(f\"An error occurred while deleting the previous data from {date_from} to {date_to}: {e}\", kind=logging.ERROR)\n",
    "    \n",
    "        for date in unique_dates:\n",
    "            group = df_file[dates == date]\n",
    "            \n",
    "            # Insert the data for the current date\n",
    "            chunk_size = 500\n",
    "            try:\n",
    "                for chunk in range(0, len(group), chunk_size):\n",
    "                    df_chunk = group.iloc[chunk:chunk + chunk_size]\n",
    "                    df_chunk.to_sql(table_name, conn, schema=db_schema, if_exists='append', index=False)\n",
    "                    conn.commit()\n",
    "                    if ((chunk+chunk_size)<len(group)): prt_info(f\"Data for {date}: {chunk+chunk_size} entries inserted.\", kind=logging.DEBUG, nl=False)\n",
    "                prt_info(f\"Data for {date} inserted successfully ({group.shape[0]} inserted).\")\n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                raise Exception(f\"An error occurred while inserting data for {date}: {e}\")\n",
    "    \n",
    "            #register the audit table\n",
    "            audit_query = text(f\"INSERT INTO \\\"AFC\\\".audit(timestamp, \\\"table\\\", operation, period, \\\"user\\\") VALUES (\\'{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\', \\'{table_name}\\', \\'insert\\', \\'{date}\\', \\'{os.getlogin()}\\')\")\n",
    "            conn.execute(audit_query)\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3216f5e-a3f2-41b5-a793-1b6ae279a5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-04_15-27-32] \n",
      "**************** Exporter version 0.41 ***********************\n",
      "[2024-09-04_15-27-32] Date 2024-09-04 15:27:32.424579\n",
      "[2024-09-04_15-27-32] Found sheet 'F07_BookingPaymentDetailed' from 'F07 (19).xlsx' as Booking Payment Detailed\n",
      "[2024-09-04_15-27-32] Found sheet 'P05-OccupancyList' from 'P05_OC_2024-08-27_to_2024-09-05_@_2024_09_04.nopag.xlsx' as Occupancy\n",
      "[2024-09-04_15-27-32] Found sheet 'P05-OccupancyList' from 'P05_OC_2024-09-06_to_2024-09-15_@_2024_09_04.nopag.xlsx' as Occupancy\n",
      "[2024-09-04_15-27-32] Found sheet 'P05-OccupancyList' from 'P05_OC_2024-09-16_to_2024-09-25_@_2024_09_04.nopag.xlsx' as Occupancy\n",
      "[2024-09-04_15-27-32] Found sheet 'P05-OccupancyList' from 'P05_OC_2024-09-26_to_2024-10-05_@_2024_09_04.nopag.xlsx' as Occupancy\n",
      "[2024-09-04_15-27-32] Found sheet 'P05-OccupancyList' from 'P05_OC_2024-10-06_to_2024-10-15_@_2024_09_04.nopag.xlsx' as Occupancy\n",
      "[2024-09-04_15-27-32] Found sheet 'P05-OccupancyList' from 'P05_OC_2024-10-16_to_2024-10-31_@_2024_09_04.nopag.xlsx' as Occupancy\n",
      "[2024-09-04_15-27-32] Found sheet 'P09_TrainList' from 'P09_TL_2024-08-27_to_2024-09-15_@_2024_09_04.nopag.xlsx' as Train List\n",
      "[2024-09-04_15-27-32] Found sheet 'P09_TrainList' from 'P09_TL_2024-09-16_to_2024-10-05_@_2024_09_04.nopag.xlsx' as Train List\n",
      "[2024-09-04_15-27-32] Found sheet 'P09_TrainList' from 'P09_TL_2024-10-06_to_2024-10-31_@_2024_09_04.nopag.xlsx' as Train List\n",
      "[2024-09-04_15-27-32] Reading Booking Payment Detailed...\n",
      "[2024-09-04_15-27-32] Sheet 'F07_BookingPaymentDetailed' from 'F07 (19).xlsx' read.\n",
      "[2024-09-04_15-27-32] Exporting Booking Payment Detailed...\n",
      "[2024-09-04_15-27-32] Previous data for 2024-09-03 deleted successfully.\n",
      "[2024-09-04_15-27-32] Data for 2024-09-03 inserted successfully (23123 inserted).\n",
      "[2024-09-04_15-27-32] Report Booking Payment Detailed exported successfully.\n",
      "[2024-09-04_15-27-32] Reading Occupancy...\n",
      "[2024-09-04_15-27-32] Sheet 'P05-OccupancyList' from 'P05_OC_2024-08-27_to_2024-09-05_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P05-OccupancyList' from 'P05_OC_2024-09-06_to_2024-09-15_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P05-OccupancyList' from 'P05_OC_2024-09-16_to_2024-09-25_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P05-OccupancyList' from 'P05_OC_2024-09-26_to_2024-10-05_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P05-OccupancyList' from 'P05_OC_2024-10-06_to_2024-10-15_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P05-OccupancyList' from 'P05_OC_2024-10-16_to_2024-10-31_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Deleting 64 duplicated entries.\n",
      "[2024-09-04_15-27-32] Exporting Occupancy...\n",
      "[2024-09-04_15-27-32] Previous data from 2024-08-27 to 2024-11-01 deleted successfully.\n",
      "[2024-09-04_15-27-32] Data for 2024-08-27 inserted successfully (556 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-28 inserted successfully (564 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-29 inserted successfully (580 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-30 inserted successfully (530 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-31 inserted successfully (550 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-01 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-02 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-03 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-04 inserted successfully (528 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-05 inserted successfully (560 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-06 inserted successfully (498 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-07 inserted successfully (530 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-08 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-09 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-10 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-11 inserted successfully (528 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-12 inserted successfully (560 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-13 inserted successfully (498 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-14 inserted successfully (530 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-15 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-16 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-17 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-18 inserted successfully (528 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-19 inserted successfully (608 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-20 inserted successfully (546 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-21 inserted successfully (578 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-22 inserted successfully (568 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-23 inserted successfully (568 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-24 inserted successfully (568 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-25 inserted successfully (576 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-26 inserted successfully (608 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-27 inserted successfully (546 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-28 inserted successfully (578 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-29 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-30 inserted successfully (520 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-01 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-02 inserted successfully (548 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-03 inserted successfully (576 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-04 inserted successfully (542 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-05 inserted successfully (514 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-06 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-07 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-08 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-09 inserted successfully (548 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-10 inserted successfully (576 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-11 inserted successfully (542 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-12 inserted successfully (514 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-13 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-14 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-15 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-16 inserted successfully (548 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-17 inserted successfully (576 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-18 inserted successfully (542 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-19 inserted successfully (514 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-20 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-21 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-22 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-23 inserted successfully (548 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-24 inserted successfully (576 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-25 inserted successfully (542 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-26 inserted successfully (514 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-27 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-28 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-29 inserted successfully (532 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-30 inserted successfully (548 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-31 inserted successfully (570 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-11-01 inserted successfully (8 inserted).\n",
      "[2024-09-04_15-27-32] Report Occupancy exported successfully.\n",
      "[2024-09-04_15-27-32] Reading Train List...\n",
      "[2024-09-04_15-27-32] Sheet 'P09_TrainList' from 'P09_TL_2024-08-27_to_2024-09-15_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P09_TrainList' from 'P09_TL_2024-09-16_to_2024-10-05_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Sheet 'P09_TrainList' from 'P09_TL_2024-10-06_to_2024-10-31_@_2024_09_04.nopag.xlsx' read.\n",
      "[2024-09-04_15-27-32] Deleting 42 duplicated entries.\n",
      "[2024-09-04_15-27-32] Exporting Train List...\n",
      "[2024-09-04_15-27-32] Previous data from 2024-08-27 to 2024-10-31 deleted successfully.\n",
      "[2024-09-04_15-27-32] Data for 2024-08-27 inserted successfully (21292 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-28 inserted successfully (24260 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-29 inserted successfully (30859 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-30 inserted successfully (25867 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-08-31 inserted successfully (31044 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-01 inserted successfully (15086 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-02 inserted successfully (14113 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-03 inserted successfully (14765 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-04 inserted successfully (11320 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-05 inserted successfully (10416 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-06 inserted successfully (5828 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-07 inserted successfully (7637 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-08 inserted successfully (4358 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-09 inserted successfully (3084 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-10 inserted successfully (3425 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-11 inserted successfully (2672 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-12 inserted successfully (3200 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-13 inserted successfully (2514 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-14 inserted successfully (3363 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-15 inserted successfully (2772 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-16 inserted successfully (2354 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-17 inserted successfully (2365 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-18 inserted successfully (2647 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-19 inserted successfully (2815 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-20 inserted successfully (2126 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-21 inserted successfully (2052 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-22 inserted successfully (1847 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-23 inserted successfully (2063 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-24 inserted successfully (1184 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-25 inserted successfully (855 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-26 inserted successfully (759 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-27 inserted successfully (552 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-28 inserted successfully (888 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-29 inserted successfully (770 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-09-30 inserted successfully (497 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-01 inserted successfully (289 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-02 inserted successfully (335 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-03 inserted successfully (354 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-04 inserted successfully (159 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-05 inserted successfully (284 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-06 inserted successfully (217 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-07 inserted successfully (321 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-08 inserted successfully (185 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-09 inserted successfully (312 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-10 inserted successfully (222 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-11 inserted successfully (127 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-12 inserted successfully (174 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-13 inserted successfully (116 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-14 inserted successfully (176 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-15 inserted successfully (160 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-16 inserted successfully (204 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-17 inserted successfully (214 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-18 inserted successfully (268 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-19 inserted successfully (163 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-20 inserted successfully (130 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-21 inserted successfully (135 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-22 inserted successfully (149 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-23 inserted successfully (105 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-24 inserted successfully (151 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-25 inserted successfully (96 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-26 inserted successfully (115 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-27 inserted successfully (148 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-28 inserted successfully (142 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-29 inserted successfully (120 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-30 inserted successfully (81 inserted).\n",
      "[2024-09-04_15-27-32] Data for 2024-10-31 inserted successfully (136 inserted).\n",
      "[2024-09-04_15-27-32] Report Train List exported successfully.\n",
      "[2024-09-04_15-27-32] Exportation finished.\n"
     ]
    }
   ],
   "source": [
    "# ************************************* MAIN PROGRAM *****************************************************\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "prt_info(f\"\\n**************** Exporter version {version} ***********************\")\n",
    "prt_info(f\"Date {datetime.datetime.today()}\")\n",
    "files_found = {}\n",
    "\n",
    "# get all xlsx files\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        # check all the sheets on the report\n",
    "        excel_file = pd.ExcelFile(file)\n",
    "\n",
    "        # loop over the sheets\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            # check if it is a report\n",
    "            first_row, kind_file = get_report_name(file, sheet_name)\n",
    "            \n",
    "            if(kind_file != NO_REPORT):\n",
    "                # get the name\n",
    "                if(kind_file == TRAIN_LIST_REPORT): kind_file_name = 'Train List'\n",
    "                elif(kind_file == BOOKING_PAYMENT_REPORT): kind_file_name = 'Booking Payment Detailed'\n",
    "                elif(kind_file == OCCUPANCY_REPORT): kind_file_name = 'Occupancy'\n",
    "                else: kind_file_name = 'Unknown'\n",
    "                prt_info(f\"Found sheet '{sheet_name}' from '{file}' as {kind_file_name}\")\n",
    "    \n",
    "                # add the file and the kind to the list\n",
    "                if not kind_file_name in files_found:\n",
    "                    files_found[kind_file_name] = []\n",
    "                    \n",
    "                if [file, sheet_name] not in files_found[kind_file_name]:\n",
    "                    files_found[kind_file_name].append([file, sheet_name])\n",
    "            \n",
    "            else: prt_info(f\"No report found in sheet '{sheet_name}' from '{file}'\", kind=logging.WARNING)\n",
    "\n",
    "        # Close the Excel file\n",
    "        excel_file.close()\n",
    "\n",
    "# if there are no files, exit\n",
    "if(len(files_found) == 0):\n",
    "    prt_info(\"No valid files found. Exiting program\", logging.WARNING)\n",
    "    sys.exit()\n",
    "\n",
    "# export each report one by one\n",
    "for report in files_found:\n",
    "    df = pd.DataFrame()\n",
    "    prt_info(f\"Reading {report}...\")\n",
    "    \n",
    "    # check all the files associated\n",
    "    for file, sheet_name in files_found[report]:\n",
    "        # reading sheet\n",
    "        try:\n",
    "            is_read = True\n",
    "            if(report == 'Train List'): df_file = read_train_list(file, alchemyEngine=alchemyEngine, sheet=sheet_name)\n",
    "            elif(report == 'Booking Payment Detailed'): df_file = read_booking_payment(file, sheet=sheet_name)\n",
    "            elif(report == 'Occupancy'): df_file = read_occupancy(file, sheet=sheet_name)            \n",
    "            else:\n",
    "                prt_info(f\"Reading of files {report} have not been implemented yet.\", logging.WARNING)\n",
    "                is_read = False\n",
    "\n",
    "            if is_read:\n",
    "                prt_info(f\"Sheet '{sheet_name}' from '{file}' read.\")\n",
    "                df = pd.concat([df, df_file])\n",
    "                \n",
    "        except Exception as e:\n",
    "            prt_info(f\"Reading of sheet '{sheet_name}' from '{file}' failed: {e}\", logging.ERROR)\n",
    "            files_found[report].remove([file, sheet_name])\n",
    "            continue\n",
    "\n",
    "    # if there is data\n",
    "    if df.empty:\n",
    "        prt_info(f'No data to export for report {report}', logging.WARNING)\n",
    "    else:\n",
    "        # order the dataframe\n",
    "        if(report == 'Train List'): sort_by = ['departure_date', 'operation_date_time']\n",
    "        elif(report == 'Booking Payment Detailed'): sort_by = ['operation_date_time']\n",
    "        elif(report == 'Occupancy'): sort_by = ['ticket_reserved', 'quota_configuration']\n",
    "            \n",
    "        df.sort_values(by= sort_by, ascending=True, inplace=True)\n",
    "    \n",
    "        # remove duplicates\n",
    "        if(report == 'Train List'): subset_col = ['ticket_number']\n",
    "        elif(report == 'Booking Payment Detailed'): subset_col = None\n",
    "        elif(report == 'Occupancy'): subset_col = ['date', 'od','train_number', 'class']\n",
    "    \n",
    "        if subset_col is not None:\n",
    "            duplicates = df.duplicated(subset=subset_col, keep='last')\n",
    "            if(duplicates.sum() > 0):\n",
    "                prt_info(f\"Deleting {duplicates.sum()} duplicated entries.\")\n",
    "                df_duplicates = df.drop_duplicates(subset=subset_col, keep='last', inplace=True, ignore_index=True)\n",
    "                \n",
    "                if not df_duplicates is None:\n",
    "                    if os.path.exists(export_folder):\n",
    "                        os.makedirs(export_folder)\n",
    "                        \n",
    "                    df_duplicates.to_csv(f\"{export_folder}/{report} duplicates {current_time}.csv.zip\")\n",
    "            \n",
    "        # export the valid files\n",
    "        prt_info(f\"Exporting {report}...\")\n",
    "        try:\n",
    "            if(report == 'Train List'):\n",
    "                export_train_list(df, alchemyEngine)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            elif(report == 'Booking Payment Detailed'):\n",
    "                export_booking_payment(df, alchemyEngine)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            elif(report == 'Occupancy'):\n",
    "                export_occupancy(df, alchemyEngine)\n",
    "                prt_info(f\"Report {report} exported successfully.\")\n",
    "            else:\n",
    "                prt_info(f\"Exportation of report {report} have not been implemented yet.\", kind=logging.WARNING)\n",
    "        except Exception as e:\n",
    "            prt_info(e)\n",
    "            prt_info(\"Exportation failed. Exportation of the report aborted.\", kind=logging.ERROR)\n",
    "            continue\n",
    "\n",
    "        # save the results\n",
    "        # Check if the directory exists, and if not, create it\n",
    "        if not os.path.exists(export_folder):\n",
    "            os.makedirs(export_folder)\n",
    "        df.to_csv(f\"{export_folder}/{report} data exported {current_time}.csv.zip\", index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "# move the original files to a subdirectory to arrange the output\n",
    "if len(files_found) > 0:\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "\n",
    "    for report in files_found:\n",
    "        for file, sheet in files_found[report]:\n",
    "            if os.path.exists(file):\n",
    "                shutil.move(file, data_folder)\n",
    "\n",
    "\n",
    "# finish\n",
    "prt_info(\"Exportation finished.\")\n",
    "\n",
    "error_file = error_handler.baseFilename\n",
    "logging.shutdown()\n",
    "\n",
    "# if there is an error, generate an alert window\n",
    "if(errors_found):\n",
    "    messagebox.showinfo(\"Alert\", \"There were ERRORS or WARNINGS during the exportation process. Please check file error_\" + log_name + \" for details.\")\n",
    "else:\n",
    "    # delete the error log\n",
    "    if os.path.exists(error_file):\n",
    "        if os.stat(error_file).st_size == 0:\n",
    "            os.remove(error_file)\n",
    "\n",
    "    messagebox.showinfo(\"Alert\", \"The exportation process has been completed SUCCESSFUL.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
